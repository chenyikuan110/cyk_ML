{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431677 ,  79\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "\n",
    "data = open('HP1.txt','r', encoding=\"utf8\").read();\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(data_size,\", \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K': 0, 'V': 1, 'l': 2, 'p': 3, ')': 4, 'g': 5, '2': 6, 't': 7, 'r': 8, 'v': 9, 's': 10, 'A': 11, 'S': 12, 'B': 13, 'm': 14, '-': 15, '\"': 16, '\\\\': 17, 'Q': 18, 'W': 19, ':': 20, '9': 21, 'e': 22, 'k': 23, 'u': 24, 'J': 25, 'y': 26, 'Z': 27, 'f': 28, '3': 29, \"'\": 30, 'w': 31, 'N': 32, '*': 33, '8': 34, 'F': 35, 'z': 36, '.': 37, '~': 38, 'G': 39, 'j': 40, 'c': 41, 'b': 42, 'U': 43, 'L': 44, 'X': 45, 'D': 46, 'a': 47, 'h': 48, 'T': 49, 'R': 50, 'o': 51, '?': 52, '6': 53, 'I': 54, 'O': 55, 'q': 56, 'H': 57, 'n': 58, '\\n': 59, 'P': 60, ' ': 61, 'C': 62, ';': 63, '4': 64, 'x': 65, 'M': 66, 'E': 67, 'd': 68, '(': 69, '1': 70, '!': 71, 'i': 72, '0': 73, '5': 74, '\\t': 75, 'Y': 76, ',': 77, '7': 78}\n",
      "{0: 'K', 1: 'V', 2: 'l', 3: 'p', 4: ')', 5: 'g', 6: '2', 7: 't', 8: 'r', 9: 'v', 10: 's', 11: 'A', 12: 'S', 13: 'B', 14: 'm', 15: '-', 16: '\"', 17: '\\\\', 18: 'Q', 19: 'W', 20: ':', 21: '9', 22: 'e', 23: 'k', 24: 'u', 25: 'J', 26: 'y', 27: 'Z', 28: 'f', 29: '3', 30: \"'\", 31: 'w', 32: 'N', 33: '*', 34: '8', 35: 'F', 36: 'z', 37: '.', 38: '~', 39: 'G', 40: 'j', 41: 'c', 42: 'b', 43: 'U', 44: 'L', 45: 'X', 46: 'D', 47: 'a', 48: 'h', 49: 'T', 50: 'R', 51: 'o', 52: '?', 53: '6', 54: 'I', 55: 'O', 56: 'q', 57: 'H', 58: 'n', 59: '\\n', 60: 'P', 61: ' ', 62: 'C', 63: ';', 64: '4', 65: 'x', 66: 'M', 67: 'E', 68: 'd', 69: '(', 70: '1', 71: '!', 72: 'i', 73: '0', 74: '5', 75: '\\t', 76: 'Y', 77: ',', 78: '7'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of input chars & indices\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# demo of onehot encoding\n",
    "vector_for_char_a = np.zeros((vocab_size,1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "N = 2;\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "# the lower the learning rate, the quicker the network abandons old belief for new input\n",
    "# e.g. train images on dogs, give a cat, low learning rate will consider cat is anormally rather than dog\n",
    "\n",
    "# Model params\n",
    "# Input to first hidden\n",
    "Wxh1 = np.random.randn(hidden_size,  vocab_size)* 0.01 # input to hidden (input is onehot encoded)\n",
    "Bxh1 = np.zeros((hidden_size,1))\n",
    "\n",
    "# Hiddens\n",
    "Whihj = np.random.randn(N-1,hidden_size, hidden_size)* 0.01 # hidden_i to hidden_i+1\n",
    "Whihi = np.random.randn(N,  hidden_size, hidden_size)* 0.01 # recurrent hidden .\n",
    "\n",
    "Bhihj = np.zeros((N-1,hidden_size,1))\n",
    "\n",
    "# last hidden to Output \n",
    "Whjy = np.random.randn(vocab_size,  hidden_size)* 0.01 # hidden to output(decode the output)\n",
    "Bhjy = np.zeros((vocab_size,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax helper\n",
    "def softmax(seq):\n",
    "    return np.exp(seq)/ np.sum(np.exp(seq))\n",
    "\n",
    "def softmax_array(two_D_seq,t):\n",
    "    return np.exp(two_D_seq[t])/ np.sum(np.exp(two_D_seq[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function - training\n",
    "def lossFunction(inputs, targets,N, prev_hiddens):\n",
    "    # p is softmax probability    \n",
    "    xs, his, ys, ps = {},{},{},{};    \n",
    "    for i in range(N):\n",
    "        his[i] = {};\n",
    "        his[i][-1] = copy.deepcopy(prev_hiddens[i])  \n",
    "    loss = 0;\n",
    "    \n",
    "    # Fwd pass    \n",
    "    for t in range(len(inputs)):\n",
    "        # One hot encoding for the input char using our dictionary\n",
    "        xs[t] = np.zeros((vocab_size,1));\n",
    "        xs[t][inputs[t]] = 1; \n",
    "        \n",
    "        his[0][t] = np.tanh(np.dot(Wxh1,xs[t]) + np.dot(Whihi[0],his[0][t-1]) + Bxh1);      \n",
    "        for i in range(1,N):\n",
    "            his[i][t] = np.tanh(np.dot(Whihj[i-1],his[i-1][t]) + np.dot(Whihi[i],his[i][t-1]) + Bhihj[i-1]);\n",
    "        \n",
    "        ys[t] = np.dot(Whjy,his[N-1][t]) + Bhjy;\n",
    "        ps[t] = softmax(ys[t]);\n",
    "        char_idx = targets[t]\n",
    "        loss += -np.log(ps[t][char_idx,0]) \n",
    "        # ps[t][targets[t]] is the prob. node corrs. to. t_th char in the label array\n",
    "\n",
    "        \n",
    "    # Gradient value holders\n",
    "    dWxh1, dWhihj, dWhjy = np.zeros_like(Wxh1),np.zeros_like(Whihj),np.zeros_like(Whjy)\n",
    "    dWhihi               = np.zeros_like(Whihi)\n",
    "    dBxh1, dBhihj, dBhjy = np.zeros_like(Bxh1),np.zeros_like(Bhihj),np.zeros_like(Bhjy)\n",
    "    dhinext = np.zeros_like(prev_hiddens)\n",
    "    \n",
    "    # Bwd pass\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        \n",
    "        # prob. p to output y\n",
    "        dy = copy.deepcopy(ps[t])\n",
    "        dy[targets[t]] -= 1 # this is how we calculate loss using onehot encoding\n",
    "        \n",
    "        # y to hj \n",
    "        dWhjy += np.dot(dy, his[N-1][t].T);        \n",
    "        dBhjy += dy # derivative w.r.t bias is 1        \n",
    "        dh_curr = np.dot(Whjy.T,dy) + dhinext[N-1] # back prop the error from y into h2\n",
    "                \n",
    "        # hj to hj-1 and hj_prev\n",
    "        # e.g. if N = 2, it needs to itr 1 time\n",
    "        # and j will start at 1\n",
    "        for j in reversed(range(1,N)):\n",
    "            dhjraw = (1-his[j][t]*his[j][t])*dh_curr # back prop thru tanh\n",
    "            dBhihj[j-1] += dhjraw  # derivative of Wx+b w.r.t b is 1; d_loss/d_b = d_loss/d_H * d_H/d_b\n",
    "            dWhihj[j-1] += np.dot(dhjraw, his[j-1][t  ].T) # derivative of Wx+b w.r.t W is x\n",
    "            dWhihi[j  ] += np.dot(dhjraw, his[j  ][t-1].T)\n",
    "            dhinext[j] = np.dot(Whihi[j].T, dhjraw)  \n",
    "            \n",
    "            dh_curr = np.dot(Whihj[j-1].T,dh_curr) + dhinext[j-1] # back prop the error from h_j into h_j-1\n",
    "        \n",
    "        # h1 to x and h1_prev\n",
    "        dhjraw = (1-his[0][t]*his[0][t])*dh_curr # back prop thru tanh\n",
    "        dBxh1     += dhjraw  \n",
    "        dWxh1     += np.dot(dhjraw,    xs[t  ].T) \n",
    "        dWhihi[0] += np.dot(dhjraw,his[0][t-1].T)\n",
    "        dhinext[0] = np.dot(Whihi[0].T, dhjraw)  \n",
    "\n",
    "    # Can be replaced using LSTM structure\n",
    "    for dparam in [dWxh1, dWhjy, dBxh1, dBhjy]:\n",
    "        np.clip(dparam,-5,5,out=dparam) # mitigate gradient vanish\n",
    "    \n",
    "    for j in range(N-1):\n",
    "        for dparam in [dWhihj[j], dWhihi[j], dBhihj[j]]:\n",
    "            np.clip(dparam,-5,5,out=dparam) # mitigate gradient vanish\n",
    "            \n",
    "    np.clip(dWhihi[N-1],-5,5,out=dWhihi[N-1]) # mitigate gradient vanish         \n",
    "    \n",
    "    h_ret = np.zeros((N,hidden_size,1))\n",
    "    for j in range(N):\n",
    "        h_ret[j] = his[j][len(inputs)-1]\n",
    "        \n",
    "        \n",
    "    return loss,dWxh1,dWhihj,dWhihi,dWhjy, dBxh1,dBhihj, dBhjy, h_ret;\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      "  esbka.eiei shshhtefekso\"asihMltare ss goC osl ngrbetyb hwnht ldrcrsehd er npfoeingn utidetoo riavn uunahenilcitieeueehwksrnOko  la hethhg, htme e w.e niick ywg . tuemelmesp hkje'dirdhwhisst ehi tynl  \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "def sample(h,hN,seed_ix,n):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # one hot encode\n",
    "    x[seed_ix] = 1;\n",
    "    ixes = [] # empty sentence\n",
    "\n",
    "    for t in range(n):\n",
    "\n",
    "        h[0] = np.tanh(np.dot(Wxh1 ,x ) + np.dot(Whihi[0],h[0]) + Bxh1 );\n",
    "        for i in range(1,hN):\n",
    "            h[i] = np.tanh(np.dot(Whihj[i-1],h[i-1]) + np.dot(Whihi[i],h[i]) + Bhihj[i-1]);\n",
    "        y = np.dot(Whjy,h[hN-1]) + Bhjy;\n",
    "        p = softmax(y);\n",
    "\n",
    "        # sample the output\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        # encode this output\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[ix] = 1;\n",
    "\n",
    "        ixes.append(ix)\n",
    "        # if n > 1, it will predict more than 1 subsequent chars\n",
    "        \n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print (\"----\\n %s \\n----\" % (txt,))\n",
    "    \n",
    "# test\n",
    "hprev = np.zeros((N,hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(hprev,N,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorc\n",
      "inputs [57, 47, 8, 8, 26, 61, 60, 51, 7, 7, 22, 8, 61, 47, 58, 68, 61, 7, 48, 22, 61, 12, 51, 8, 41]\n",
      "arry Potter and the Sorce\n",
      "targets [47, 8, 8, 26, 61, 60, 51, 7, 7, 22, 8, 61, 47, 58, 68, 61, 7, 48, 22, 61, 12, 51, 8, 41, 22]\n"
     ]
    }
   ],
   "source": [
    "# Training using Adagrad (decreasing learning rate)\n",
    "position = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "print(data[position:position+seq_length])\n",
    "print(\"inputs\",inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "print(data[position+1:position+seq_length+1])\n",
    "print(\"targets\",targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 109.20401781297652\n",
      "----\n",
      " eav ornhws peyna h eciH omwba rytlcccdc lcoonyrShigueotnsly\n",
      " hgwslheo istn c ns w  i  esyien nelgeednWtma'roat oaaittn.wagn hde  r nila t delewt stpso.e e'mtero:n od\n",
      "fttla iaabh mtrviml as- adeTeoeyoe \n",
      "----\n",
      "1000 loss 93.87083013834888\n",
      "----\n",
      " Mu,ttol Di;ge wrtDoShoia htirdhotrotk  wfe we dvPelpg o sei  g l h  fmbawFet Gyytdlte,ns gaar usgtl ogee,getndcaeo u areEcwef hwhe aoue fpitHtVt sdunewesl uer\\W ate iehnler  ezntlmi ,torio  gdyloav,dr \n",
      "----\n",
      "2000 loss 82.83375848778253\n",
      "----\n",
      " hoesa ue?vebs jiyf t'aipoene aos eed hrinlgb,l teee thrnhrrid,es, nd he inera\" rh hHnorg h\"s,drHirv hl oh VO' tBrlmeer nhd her ant stt HAneVe hud tus eihi br. lwp Hs. aen ph gunfrs ictDllsaasir yust E \n",
      "----\n",
      "3000 loss 77.80788808127976\n",
      "----\n",
      " l hii. wi lrp. hinh. coe -irb\n",
      "sdet,ir,E iowny\"sikmr - gf'ne, oDmard \"oYetiParl ton.!. wk aous tun todhyl\" 7hvl. Putetd Wuplnoud yisedrr mnlfindei nnmR aaaeetleut,s vow de iu pmG,im\" meone spo, shnr UD \n",
      "----\n",
      "4000 loss 75.13541300062374\n",
      "----\n",
      " uard?a?ue,,ouay\" locubmypoter Mncuhag Olsd bsgid thl se Ium ah aile's'y nochezoHilv silnain He'p ug plait- afrdswan nanitlerlt rt hs -old hll 'aalllee gh gat uarphneeey ba-has p-ets suancH1pp aaler\n",
      "hn \n",
      "----\n",
      "5000 loss 71.60591810208292\n",
      "----\n",
      " nae li farey Hmme cehocsarp. so tht ne Viriv pinlt Hacho biAt \"Ie wa oalelr'e.\n",
      "homlllsand \n",
      "he tt Hrtoulalallanthvcarec lhry ouin,.\" an one. wer diging adep t tost nrlired niv.hem kmn Os wii we thk ig  \n",
      "----\n",
      "6000 loss 68.88196913984788\n",
      "----\n",
      " lan'ard tith hod igibtor styot Hho pet toheon,\n",
      "NAIt\"\n",
      "\"ite kmy ig wot kims oquilgunn- on, we uot- aprto, eas\"O6heny\" ar gory sot-Knws\n",
      "-Wuig tias lat,. Th.s huden Hery g ar ieo, mos'le sok wult Rurest t \n",
      "----\n",
      "7000 loss 67.19763144738194\n",
      "----\n",
      "  we booe jas tI bit be ppaleachan- att herey ar,:, Ier yhopinr tle lideke ae og,one anne thon'e yo hdle,es, Hyer fot arrn gafhld oanw, Iote aret, Tse ?oIr hoy to! feb\"we pawicte dy ifl heog toagsoveg, \n",
      "----\n",
      "8000 loss 65.82138399117645\n",
      "----\n",
      "  outhiste sear a Aun berutes tepe mooks\" aocherin'tsyhy.y \\tres a'vomeiv p, ate rasmf Haer linglit.e fit ernle sot ovandonnadornr tand. Thothin Ro . anlibes Sit waan lind se t. beons\"\"\"-\"\n",
      "\"Halerserey  \n",
      "----\n",
      "9000 loss 64.90787581716286\n",
      "----\n",
      " nd ipveled apdetgs ti it-\n",
      "WOumed-d, ghecbo backeto, qoetey seitredher rkooj We fiicwarul boithirl ig ware gampset heoey tt. Hw,otledm shate,.WHErye Canis -fanb, 's,ty de cugly soqhenand Sqt thotank wa \n",
      "----\n",
      "10000 loss 63.66818962736845\n",
      "----\n",
      " \"\n",
      "\"hor' oye tho toaos mrid! Halk, vid ameots f the whof,h hor os rlpthom ut otenk ste,k the hor the seoeey lhune ther stoy Inve aum? Haulen dit tlene Hiinn fol a yn dautrd'ot oNu?aw ce d'ncoye uer lle \n",
      "----\n",
      "11000 loss 63.92671440977749\n",
      "----\n",
      " it th yal mey to eod goas hou',?\"\n",
      "Hapsir wreys, almiignt He ofhn hassuof bunlad fugkcanr -we'okelesid, mouvey ald ang, thanis Goud:\"\"\n",
      "I Mrarinn the faug, bis toorn t arswaco jerte.\". Hny ture. THaaged \n",
      "----\n",
      "12000 loss 62.24930080880576\n",
      "----\n",
      " ed andrind.h ot goudt.\" the Toound dawl wlcfont, to the inde helm youndtanb,t worer ysing to bmou, fiikeit fas ohelavirea thimerer hid.\"\n",
      "Aovry hoeic? balyed a de mas in's. Tit toblny fit Crcheth on hi \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-765dd15c0b95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchar_to_ix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWxh1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWhihj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWhihi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWhjy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdBxh1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdBhihj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdBhjy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-11-a7a0591619b3>\u001b[0m in \u001b[0;36mlossFunction\u001b[1;34m(inputs, targets, N, prev_hiddens)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mhis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWxh1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhihi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBxh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m             \u001b[0mhis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhihj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhihi\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBhihj\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhjy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhis\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBhjy\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Real training\n",
    "\n",
    "n, position = 0,0\n",
    "\n",
    "# for Adaptive Gradient descent\n",
    "mWxh1 = np.zeros_like(Wxh1);\n",
    "mWhihi = np.zeros_like(Whihi);\n",
    "mWhihj = np.zeros_like(Whihj);\n",
    "mWhjy = np.zeros_like(Whjy);\n",
    "\n",
    "mBxh1 = np.zeros_like(Bxh1);\n",
    "mBhihj = np.zeros_like(Bhihj);\n",
    "mBhjy = np.zeros_like(Bhjy);\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length;\n",
    "\n",
    "epoch = 200*1000;\n",
    "sample_length = 200;\n",
    "hprev = np.zeros((N,hidden_size,1))\n",
    "\n",
    "while n<epoch:\n",
    "\n",
    "    if(position+seq_length+1 >= len(data) or n == 0):\n",
    "        \n",
    "        hprev = np.zeros_like(hprev)\n",
    "        position = 0;\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "    \n",
    "    loss,dWxh1,dWhihj,dWhihi,dWhjy, dBxh1,dBhihj, dBhjy, hprev = lossFunction(inputs,targets,N,hprev)  \n",
    "    smooth_loss = smooth_loss*0.999+loss*0.001\n",
    "    \n",
    "    if(n%1000 == 0):\n",
    "        print(n,\"loss\",smooth_loss)\n",
    "        sample(hprev,N,inputs[0],sample_length);\n",
    "        \n",
    "    # update\n",
    "    for param, dparam, mem in zip([Wxh1,Bxh1,Whjy,Bhjy],\n",
    "                                  [dWxh1,dBxh1,dWhjy,dBhjy],\n",
    "                                  [mWxh1,mBxh1,mWhjy,mBhjy]):\n",
    "        \n",
    "        mem += dparam*dparam;\n",
    "        param += -learning_rate * dparam *  1 / np.sqrt(mem +1e-8) # Adagrad\n",
    "    \n",
    "    for i in range(N-1):\n",
    "        for param, dparam, mem in zip([ Whihj[i], Whihi[i], Bhihj[i]],\n",
    "                                      [dWhihj[i],dWhihi[i],dBhihj[i]],\n",
    "                                      [mWhihj[i],mWhihi[i],mBhihj[i]]):\n",
    "            mem += dparam*dparam;\n",
    "            param += -learning_rate * dparam *  1 / np.sqrt(mem +1e-8) # Adagrad    \n",
    "    \n",
    "    mWhihi[N-1] += dWhihi[N-1]*dWhihi[N-1];\n",
    "    Whihi[N-1] += -learning_rate * dWhihi[N-1] *  1 / np.sqrt(mWhihi[N-1] +1e-8) # Adagrad    \n",
    "        \n",
    "    position += seq_length;\n",
    "    n += 1;\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
