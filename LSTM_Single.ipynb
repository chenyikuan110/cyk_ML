{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431677 ,  79\n",
      "{'3': 0, 'g': 1, 'z': 2, 'j': 3, 'N': 4, '\\\\': 5, 'q': 6, 'O': 7, 'B': 8, 'v': 9, 'U': 10, 'Y': 11, '8': 12, '4': 13, 'e': 14, '6': 15, 'm': 16, 'f': 17, 'y': 18, '-': 19, '9': 20, ':': 21, 'G': 22, 'H': 23, 'k': 24, 'i': 25, 'C': 26, 'L': 27, ' ': 28, 'A': 29, '~': 30, 'F': 31, 'o': 32, 'S': 33, 'd': 34, 'J': 35, 's': 36, '1': 37, 'P': 38, ';': 39, 'E': 40, 'K': 41, 'r': 42, 'l': 43, 'D': 44, '!': 45, 'p': 46, '\\n': 47, 'I': 48, ',': 49, 'X': 50, 'b': 51, '5': 52, 'a': 53, '\"': 54, 'Q': 55, '2': 56, 'h': 57, 'R': 58, 'n': 59, \"'\": 60, '.': 61, 'x': 62, 'Z': 63, '*': 64, '?': 65, '0': 66, '(': 67, 't': 68, 'V': 69, 'M': 70, ')': 71, 'T': 72, 'W': 73, '\\t': 74, 'u': 75, '7': 76, 'c': 77, 'w': 78}\n",
      "{0: '3', 1: 'g', 2: 'z', 3: 'j', 4: 'N', 5: '\\\\', 6: 'q', 7: 'O', 8: 'B', 9: 'v', 10: 'U', 11: 'Y', 12: '8', 13: '4', 14: 'e', 15: '6', 16: 'm', 17: 'f', 18: 'y', 19: '-', 20: '9', 21: ':', 22: 'G', 23: 'H', 24: 'k', 25: 'i', 26: 'C', 27: 'L', 28: ' ', 29: 'A', 30: '~', 31: 'F', 32: 'o', 33: 'S', 34: 'd', 35: 'J', 36: 's', 37: '1', 38: 'P', 39: ';', 40: 'E', 41: 'K', 42: 'r', 43: 'l', 44: 'D', 45: '!', 46: 'p', 47: '\\n', 48: 'I', 49: ',', 50: 'X', 51: 'b', 52: '5', 53: 'a', 54: '\"', 55: 'Q', 56: '2', 57: 'h', 58: 'R', 59: 'n', 60: \"'\", 61: '.', 62: 'x', 63: 'Z', 64: '*', 65: '?', 66: '0', 67: '(', 68: 't', 69: 'V', 70: 'M', 71: ')', 72: 'T', 73: 'W', 74: '\\t', 75: 'u', 76: '7', 77: 'c', 78: 'w'}\n"
     ]
    }
   ],
   "source": [
    "data = open('HP1.txt','r', encoding=\"utf8\").read();\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(data_size,\", \",vocab_size)\n",
    "\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)\n",
    "\n",
    "def encode(idx,num_entry):\n",
    "    ret = np.zeros(num_entry)\n",
    "    ret[idx] = 1\n",
    "    return ret;\n",
    "\n",
    "def encode_array(array,num_entry):\n",
    "    xs = np.zeros((len(array),num_entry))\n",
    "    for i in range(len(array)):\n",
    "        xs[i][array[i]] = 1; \n",
    "    return xs;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Helper functions\n",
    "def softmax(array):\n",
    "    return np.exp(array)/ np.sum(np.exp(array)) # return an array\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def sigmoid_deriv(y):\n",
    "    return (y*(1-y))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(y):\n",
    "    return 1 - pow(np.tanh(y),2)\n",
    "\n",
    "# RNN\n",
    "class myRNN:\n",
    "    \n",
    "    def __init__ (self, lenIn, lenOut, lenRec, sizeHidden, inputs_encoded, targets, learningRate):\n",
    "        \n",
    "        # Hyper parameters\n",
    "        self.lenIn          = lenIn\n",
    "        self.lenOut         = lenOut\n",
    "        self.lenRec         = lenRec\n",
    "        self.sizeHidden     = sizeHidden\n",
    "        self.learningRate   = learningRate\n",
    "        \n",
    "        # input & expected output\n",
    "        self.inputs_encoded = inputs_encoded;\n",
    "        self.targets = targets;\n",
    "        \n",
    "        # parameters for inference\n",
    "        self.x  = np.zeros(lenIn)  \n",
    "        self.y  = np.zeros(lenOut)\n",
    "        self.h  = np.zeros(sizeHidden)\n",
    "        self.c  = np.zeros(sizeHidden)\n",
    "        \n",
    "        self.W  = np.zeros((lenOut,sizeHidden)) # for the last fully connected layer\n",
    "        self.GW = np.zeros((lenOut,sizeHidden)) # Gradient, for W-update using RMSprop\n",
    "        self.b  = np.zeros(lenOut)\n",
    "        self.Gb = np.zeros(lenOut)\n",
    "        \n",
    "        # for training phase \n",
    "        self.xs = np.zeros((lenRec,lenIn))\n",
    "        self.ys = np.zeros((lenRec,lenOut))\n",
    "        self.cs = np.zeros((lenRec,sizeHidden))\n",
    "        self.hs = np.zeros((lenRec,sizeHidden))\n",
    "        \n",
    "        # for training phase bookkeeping\n",
    "        self.fg = np.zeros((lenRec,sizeHidden)) # forget gate\n",
    "        self.ig = np.zeros((lenRec,sizeHidden)) # input  gate\n",
    "        self.og = np.zeros((lenRec,sizeHidden)) # output gate\n",
    "        self.mc = np.zeros((lenRec,sizeHidden)) # memory cell state (candidate)\n",
    "        \n",
    "        # LSTM class\n",
    "        self.LSTM = LSTM(sizeHidden+lenIn,sizeHidden,lenRec,learningRate)\n",
    "        \n",
    "        ''' end of myRNN.__init__ '''\n",
    "       \n",
    "    ''' This is used when mini-batch is used '''            \n",
    "    def update_inputs_targets(self, inputs_encoded, targets):\n",
    "        self.inputs_encoded  = inputs_encoded\n",
    "        self.targets         = targets\n",
    "    \n",
    "    def fwd_pass(self): \n",
    "        prev_h = np.zeros_like(self.hs[0])\n",
    "        for t in range(0,self.lenRec):\n",
    "            # update input\n",
    "            self.x    = self.inputs_encoded[t]\n",
    "            self.xs[t]= self.inputs_encoded[t]\n",
    "            \n",
    "            self.LSTM.hx = np.hstack((prev_h, self.x));\n",
    "           \n",
    "            c, h, f, i, m, o = self.LSTM.fwd_pass()\n",
    "            # bookkeeping\n",
    "            self.cs[t] = c\n",
    "            self.hs[t] = h\n",
    "            self.fg[t] = f\n",
    "            self.ig[t] = i\n",
    "            self.mc[t] = m\n",
    "            self.og[t] = o\n",
    "            \n",
    "            # output layer - fully connected layer\n",
    "            self.ys[t] = np.dot(self.W,self.hs[t]) + self.b\n",
    "            prev_h = self.hs[t]\n",
    "            \n",
    "        return;              \n",
    "    \n",
    "    def bwd_pass(self):        \n",
    "\n",
    "        avg_loss = 0; # using cross entropy average\n",
    "        c2next_grad  = np.zeros(self.sizeHidden)\n",
    "        h2next_grad  = np.zeros(self.sizeHidden)\n",
    "        \n",
    "        # output bp\n",
    "        W_grad   = np.zeros((self.lenOut,self.sizeHidden))\n",
    "        b_grad  = np.zeros(self.lenOut)\n",
    "        \n",
    "        # LSTM internal bp\n",
    "        hxf_grad   = np.zeros((self.sizeHidden,self.LSTM.lenIn));\n",
    "        hxi_grad   = np.zeros((self.sizeHidden,self.LSTM.lenIn));\n",
    "        hxm_grad   = np.zeros((self.sizeHidden,self.LSTM.lenIn));\n",
    "        hxo_grad   = np.zeros((self.sizeHidden,self.LSTM.lenIn));\n",
    "    \n",
    "        fb_grad   = np.zeros(self.sizeHidden)\n",
    "        ib_grad   = np.zeros(self.sizeHidden)\n",
    "        mb_grad   = np.zeros(self.sizeHidden)\n",
    "        ob_grad   = np.zeros(self.sizeHidden)\n",
    "                   \n",
    "        # propagates through time and layers\n",
    "\n",
    "        for t in reversed(range(0,self.lenRec)):\n",
    "            \n",
    "            prob = softmax(self.ys[t]) # prevent zero\n",
    "            prob_fix  = prob + 1e-9\n",
    "#            if(prob[self.targets[t]] == 0):\n",
    "#                for ii in range(10000):\n",
    "#                    print(\"ERR!\",self.ys[t][self.targets[t]],\" \",np.sum(self.ys[t]))\n",
    "                \n",
    "            # cross entropy\n",
    "            err       = np.log(prob_fix[self.targets[t]])\n",
    "            avg_loss += err\n",
    "     \n",
    "            dy = copy.deepcopy(prob)\n",
    "            dy[self.targets[t]] -= 1\n",
    "            \n",
    "            W_grad += np.dot((np.atleast_2d(dy)).T,np.atleast_2d(self.hs[t]))\n",
    "            b_grad += dy\n",
    "            \n",
    "            dh = np.dot(self.W.T,dy) + h2next_grad\n",
    "            \n",
    "            x_grad  = np.zeros(self.lenIn)\n",
    "            \n",
    "            if(t > 0):\n",
    "                prev_h = self.hs[t-1]\n",
    "                prev_c = self.cs[t-1]\n",
    "            else:\n",
    "                prev_h = np.zeros_like(self.hs[0])\n",
    "                prev_c = np.zeros_like(self.cs[0])\n",
    "                \n",
    "            self.LSTM.hx = np.hstack((prev_h,self.xs[t]));\n",
    "            self.LSTM.c = self.cs[t];\n",
    "\n",
    "            dhxf,dhxi,dhxm,dhxo, dbf,dbi,dbm,dbo, c2next_grad,h2next_grad,x_grad = \\\n",
    "            self.LSTM.bwd_pass( dh, prev_c ,self.fg[t],self.ig[t],self.mc[t],self.og[t],\\\n",
    "                                 c2next_grad);\n",
    "            \n",
    "            hxf_grad +=  dhxf;\n",
    "            hxi_grad +=  dhxi;  \n",
    "            hxm_grad +=  dhxm;  \n",
    "            hxo_grad +=  dhxo;   \n",
    "            fb_grad +=  dbf;\n",
    "            ib_grad +=  dbi;\n",
    "            mb_grad +=  dbm;\n",
    "            ob_grad +=  dbo;\n",
    "\n",
    "        \n",
    "        # update using RMSprop\n",
    "        self.LSTM.update(hxf_grad/self.lenRec, hxi_grad/self.lenRec, \\\n",
    "                           hxm_grad/self.lenRec, hxo_grad/self.lenRec, \\\n",
    "                           fb_grad/self.lenRec, ib_grad/self.lenRec, \\\n",
    "                           mb_grad/self.lenRec, ob_grad/self.lenRec);\n",
    "\n",
    "        \n",
    "        self.update(W_grad/self.lenRec,b_grad/self.lenRec);\n",
    "        return avg_loss/self.lenRec;\n",
    "            \n",
    "          \n",
    "            \n",
    "    def update(self, W_grad, b_grad):\n",
    "        self.GW = 0.9*self.GW + 0.1*W_grad**2;\n",
    "        self.W -= self.learningRate/np.sqrt(self.GW + 1e-8) * W_grad;\n",
    "        self.Gb = 0.9*self.Gb + 0.1*b_grad**2;\n",
    "        self.b -= self.learningRate/np.sqrt(self.Gb + 1e-8) * b_grad;\n",
    "\n",
    "    def inference(self,x):\n",
    "        # update input\n",
    "        self.x = x\n",
    "        self.LSTM.hx = np.hstack((self.h,self.x))\n",
    "        c, h, f, i, m, o = self.LSTM.fwd_pass()\n",
    "        self.c = c\n",
    "        self.h = h\n",
    "\n",
    "        # output layer - may replace with softmax instead\n",
    "        self.y = np.dot(self.W,self.h) + self.b\n",
    "        p   = softmax(self.y)     \n",
    "        return np.random.choice(range(self.lenOut), p=p.ravel())\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    \n",
    "    def __init__ (self,lenIn,sizeHidden,lenRec,learningRate):\n",
    "        self.lenIn        = lenIn\n",
    "        self.sizeHidden   = sizeHidden\n",
    "        self.lenRec       = lenRec\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "        # hx == x is x and h horizontally stacked together\n",
    "        self.hx = np.zeros(lenIn)\n",
    "        self.c = np.zeros(sizeHidden)\n",
    "        self.h = np.zeros(sizeHidden)\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.fW = np.random.random((sizeHidden,lenIn));\n",
    "        self.iW = np.random.random((sizeHidden,lenIn));\n",
    "        self.mW = np.random.random((sizeHidden,lenIn)); # cell state\n",
    "        self.oW = np.random.random((sizeHidden,lenIn));\n",
    "                             \n",
    "        # biases\n",
    "        self.fb = np.zeros(sizeHidden);\n",
    "        self.ib = np.zeros(sizeHidden); \n",
    "        self.mb = np.zeros(sizeHidden); \n",
    "        self.ob = np.zeros(sizeHidden); \n",
    "               \n",
    "        # for RMSprop only\n",
    "        self.GfW = np.random.random((sizeHidden,lenIn));\n",
    "        self.GiW = np.random.random((sizeHidden,lenIn));\n",
    "        self.GmW = np.random.random((sizeHidden,lenIn)); \n",
    "        self.GoW = np.random.random((sizeHidden,lenIn));\n",
    "                             \n",
    "        self.Gfb = np.zeros(sizeHidden);\n",
    "        self.Gib = np.zeros(sizeHidden); \n",
    "        self.Gmb = np.zeros(sizeHidden);\n",
    "        self.Gob = np.zeros(sizeHidden); \n",
    "        \n",
    "        ''' end of LSTM.__init__ '''\n",
    "        \n",
    "    def fwd_pass(self):\n",
    "        f       = sigmoid(np.dot(self.fW, self.hx) + self.fb)\n",
    "        i       = sigmoid(np.dot(self.iW, self.hx) + self.ib)\n",
    "        m       = tanh(   np.dot(self.mW, self.hx) + self.mb)        \n",
    "        o       = sigmoid(np.dot(self.oW, self.hx) + self.ob)\n",
    "        self.c *= f\n",
    "        self.c += i * m\n",
    "        self.h  = o * tanh(self.c)\n",
    "        \n",
    "        return self.c, self.h, f, i, m, o;\n",
    "    \n",
    "    def bwd_pass(self, dh, prev_c, f, i, m, o, c_g):\n",
    "        \n",
    "        #dh = np.clip(dh, -6, 6);       \n",
    "        # h = o*tanh(c)\n",
    "        do  = tanh(self.c) * dh\n",
    "        do  = sigmoid_deriv(o)*do\n",
    "        dhxo = np.dot((np.atleast_2d(do)).T,np.atleast_2d(self.hx)) \n",
    "        \n",
    "        # h = o*tanh(c) - add c_g (c_grad in next timestep, account for the branch here)\n",
    "        dcs = dh * o * tanh_deriv(self.c) + c_g\n",
    "        #dcs = np.clip(dcs, -6, 6); \n",
    "        \n",
    "        # c = c_prev * f + m * i\n",
    "        dm = i * dcs\n",
    "        dm = tanh_deriv(m) * dm\n",
    "        dhxm = np.dot((np.atleast_2d(dm)).T,np.atleast_2d(self.hx)) \n",
    "        \n",
    "        # c = c_prev * f + m * i\n",
    "        di  = m * dcs\n",
    "        di  = sigmoid_deriv(i) * di\n",
    "        dhxi = np.dot((np.atleast_2d(di)).T,np.atleast_2d(self.hx)) \n",
    "        \n",
    "        # c = c_prev * f + m * i\n",
    "        df = prev_c * dcs\n",
    "        df = sigmoid_deriv(f) * df\n",
    "        dhxf = np.dot((np.atleast_2d(df)).T,np.atleast_2d(self.hx)) \n",
    "        \n",
    "        # c = c_prev * f + m * i\n",
    "        c_grad  = dcs * f\n",
    "        hx_grad = np.dot(self.fW.T, df) + np.dot(self.iW.T, di) +\\\n",
    "                          np.dot(self.oW.T, do) + np.dot(self.mW.T, dm)\n",
    "        \n",
    "        \n",
    "        return dhxf,dhxi,dhxm,dhxo,df,di,dm,do,c_grad,hx_grad[:self.sizeHidden],hx_grad[self.sizeHidden:];\n",
    "    \n",
    "    def update(self, f_grad, i_grad, m_grad, o_grad, fb_grad, ib_grad, mb_grad, ob_grad):\n",
    "\n",
    "        self.GfW = 0.9*self.GfW + 0.1*f_grad**2\n",
    "        self.GiW = 0.9*self.GiW + 0.1*i_grad**2\n",
    "        self.GmW = 0.9*self.GmW + 0.1*m_grad**2\n",
    "        self.GoW = 0.9*self.GoW + 0.1*o_grad**2\n",
    "        \n",
    "        self.Gfb = 0.9*self.Gfb + 0.1*fb_grad**2\n",
    "        self.Gib = 0.9*self.Gib + 0.1*ib_grad**2\n",
    "        self.Gmb = 0.9*self.Gmb + 0.1*mb_grad**2\n",
    "        self.Gob = 0.9*self.Gob + 0.1*ob_grad**2\n",
    "        \n",
    "        self.fW -= self.learningRate/np.sqrt(self.GfW + 1e-8) * f_grad\n",
    "        self.iW -= self.learningRate/np.sqrt(self.GiW + 1e-8) * i_grad\n",
    "        self.mW -= self.learningRate/np.sqrt(self.GmW + 1e-8) * m_grad\n",
    "        self.oW -= self.learningRate/np.sqrt(self.GoW + 1e-8) * o_grad\n",
    "        self.fb -= self.learningRate/np.sqrt(self.Gfb + 1e-8) * fb_grad\n",
    "        self.ib -= self.learningRate/np.sqrt(self.Gib + 1e-8) * ib_grad\n",
    "        self.mb -= self.learningRate/np.sqrt(self.Gmb + 1e-8) * mb_grad\n",
    "        self.ob -= self.learningRate/np.sqrt(self.Gob + 1e-8) * ob_grad\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorcerer's Stone\n",
      "CHAPTER ONE\n",
      "THE BOY WHO LIVED\n",
      "Mr. and\n",
      "inputs [23, 53, 42, 42, 18, 28, 38, 32, 68, 68, 14, 42, 28, 53, 59, 34, 28, 68, 57, 14, 28, 33, 32, 42, 77, 14, 42, 14, 42, 60, 36, 28, 33, 68, 32, 59, 14, 47, 26, 23, 29, 38, 72, 40, 58, 28, 7, 4, 40, 47, 72, 23, 40, 28, 8, 7, 11, 28, 73, 23, 7, 28, 27, 48, 69, 40, 44, 47, 70, 42, 61, 28, 53, 59, 34]\n",
      "arry Potter and the Sorcerer's Stone\n",
      "CHAPTER ONE\n",
      "THE BOY WHO LIVED\n",
      "Mr. and \n",
      "targets [53, 42, 42, 18, 28, 38, 32, 68, 68, 14, 42, 28, 53, 59, 34, 28, 68, 57, 14, 28, 33, 32, 42, 77, 14, 42, 14, 42, 60, 36, 28, 33, 68, 32, 59, 14, 47, 26, 23, 29, 38, 72, 40, 58, 28, 7, 4, 40, 47, 72, 23, 40, 28, 8, 7, 11, 28, 73, 23, 7, 28, 27, 48, 69, 40, 44, 47, 70, 42, 61, 28, 53, 59, 34, 28]\n",
      "!!!! 431677\n",
      "0 err: -4.36944777346702\n",
      "3\n",
      "BHatEaSInEItotO rSTnOoedVO tE\n",
      "e\n",
      "STaTTc\n",
      "e\n",
      "500 err: -2.1475507015119693\n",
      "C\n",
      "heys Uncollys aldn't dooirs it saOned fi\n",
      "1000 err: -2.0892386680919595\n",
      "5\n",
      "wherm co lot whe shesthisawat und smat, \n",
      "1500 err: -2.1968226596876406\n",
      "w\n",
      "ave tooks wheras and Ley sat 't inlaidte\n",
      "2000 err: -1.7689480838767209\n",
      "i\n",
      "ns abutsking forthe hene in fothen'd fle\n",
      "2500 err: -2.1467696876625255\n",
      "b\n",
      "oor asfed wastings,ton'll lichist whit c\n",
      "3000 err: -2.0929241417157116\n",
      "c\n",
      "e to ened thed was for do vos une sthey \n",
      "3500 err: -2.139974445058305\n",
      "k\n",
      "edmed tich hiad using han and Hardled co\n",
      "4000 err: -2.068105968182464\n",
      "X\n",
      "ed, but medwo,\" gee. \"If ou.\n",
      "\"Mckte, go,\n",
      "4500 err: -2.2847727892113165\n",
      "7\n",
      "nloMuld d an's hestpend crofesing yoff  \n",
      "5000 err: -1.8081465170309798\n",
      "H\n",
      "an'd what an he -Master ane thill relisl\n",
      "5500 err: -2.4136841545689522\n",
      ",\n",
      "\" Hetich ust hred fleeld rmp dis air bri\n",
      "!!!! 431677\n",
      "6000 err: -2.3549497291850368\n",
      "u\n",
      "ghy's beckething alllecoverdoh.\"\n",
      "\"-ar th\n",
      "6500 err: -2.2128988550751947\n",
      "G\n",
      "enst, red onyure we croom.\n",
      "Uncle  setott\n",
      "7000 err: -4.651453155985276\n",
      "I\n",
      "Dl S- The. Gorlyd, w us as a youl Whad D\n",
      "7500 err: -2.271755267992762\n",
      "\t\n",
      "ckeeret lllands at, toed eiryoo ar fre t\n",
      "8000 err: -11.344894466111445\n",
      ":\n",
      "'m lornorrnene atirlorne onl, the arlosh\n",
      "8500 err: -9.48826087128061\n",
      "\n",
      "\n",
      "\"on  dasmikd..'ikkrmis.....'ilkndnrigaga\n",
      "9000 err: -5.004509144525257\n",
      "W\n",
      "\n",
      "- ciux -ndal, ittiI pintnglnd. sningtm \n",
      "9500 err: -4.216771125709836\n",
      "9\n",
      " tr. dwis owetoone wwnuoowes Ropaoote ou\n",
      "10000 err: -3.972007920087017\n",
      "p\n",
      "eerr. nrirg nngtoutu Hant he. khe tnnnt \n",
      "10500 err: -4.222556915336095\n",
      "T\n",
      "hemtahettrolg ohe oaragehangkhawauoasoho\n",
      "11000 err: -3.0667831151931737\n",
      "-\n",
      "-\"antpee g cer nheceropa.\n",
      "\"e tpacogree k\n",
      "11500 err: -3.3783554075811892\n",
      "!\n",
      "i,\"ininycksaiiin indy cot min,\n",
      "sf perr g\n",
      "!!!! 431677\n",
      "12000 err: -3.8563164284664824\n",
      ")\n",
      " t\"en hey antte aoU ring. oo(etooloo(ou\n",
      "\n",
      "12500 err: -3.8339138832909976\n",
      "y\n",
      " ald yrues I\"nd  e  e t \"tepeme \"le z ls\n",
      "13000 err: -5.426582466632866\n",
      "D\n",
      "huus wh tooyoa\"s -oorourouamloasos\n",
      "\"\n",
      "\"\"s\n",
      "13500 err: -3.617600407611899\n",
      "M\n",
      "y heentrntdey t,lerrtehigetnuttdoditeltd\n",
      "14000 err: -3.5767796489500383\n",
      "f\n",
      "e onledeofeenheewesvyerredoa wsheeeeseee\n",
      "14500 err: -3.856098844517929\n",
      "l\n",
      "s tlnam sin tsiting tengg igrtitri fror \n",
      "15000 err: -2.9806386569211467\n",
      "V\n",
      "om to gle lbw to t  orrs --- bttprsty sc\n",
      "15500 err: -4.22341285945363\n",
      "m\n",
      "Ls\n",
      "dthysesng d as ouch\"nsec h dssAsuh  s\n",
      "16000 err: -3.4823788894227854\n",
      "r\n",
      " Mue\" iuihiwrinw wiiyilibp iuliiiil\"\"ivi\n",
      "16500 err: -4.228745117503867\n",
      "t\n",
      "ish  baa d d.aaaaand ta ba iaaohy and a'\n",
      "17000 err: -4.269108160912712\n",
      "6\n",
      "s  ao j c   t   boht tt  Qid b re  eaat \n",
      "!!!! 431677\n",
      "17500 err: -4.446943009451087\n",
      "K\n",
      "\n",
      "on\"kovfcor nofrhlookn ykookannhoosiorak\n",
      "18000 err: -3.3654043600534287\n",
      "(\n",
      "rhlt tuutl  Domme f fse  Plf, DoV tfedcT\n",
      "18500 err: -4.214282721541072\n",
      "e\n",
      " yet PhsPseF intTes.se'ss nes tnunlsrtom\n",
      "19000 err: -3.7098494206358685\n",
      "E\n",
      ". le.rol hIqii eliweesone.hpede ipeiolpi\n",
      "19500 err: -4.239674518601751\n",
      "0\n",
      "l\" nf\"a tfa nnd st\"r akrlarloinya\"aoinal\n"
     ]
    }
   ],
   "source": [
    "seq_length,position = 75,0\n",
    "inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "print(data[position:position+seq_length])\n",
    "print(\"inputs\",inputs)\n",
    "\n",
    "targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "print(data[position+1:position+seq_length+1])\n",
    "print(\"targets\",targets)\n",
    "\n",
    "n,position = 0,0;\n",
    "epoch = 20*1000;\n",
    "lenIn, lenOut, lenRec = vocab_size,vocab_size, seq_length\n",
    "sizeHidden, numHiddenLayer = 100,1;\n",
    "learningRate = 0.1;\n",
    "\n",
    "\n",
    "R = myRNN(lenIn, lenOut, lenRec, sizeHidden, encode_array(inputs,vocab_size),targets, learningRate)\n",
    "\n",
    "# training\n",
    "while n<epoch:\n",
    "    \n",
    "    if(position+seq_length+1 >= len(data) or n == 0):\n",
    "        print(\"!!!!\",len(data))\n",
    "        position = 0;\n",
    "        \n",
    "    inputs  = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "\n",
    "    R.update_inputs_targets(encode_array(inputs,vocab_size),targets)\n",
    "    R.fwd_pass();\n",
    "    \n",
    "    err = R.bwd_pass();\n",
    "    \n",
    "    if(n%500 == 0):\n",
    "        print(n,\"err:\",err)\n",
    "        seed = encode(n % vocab_size,vocab_size)\n",
    "        print(ix_to_char[n % vocab_size])\n",
    "        result = [];\n",
    "        R.h  = np.zeros(sizeHidden)\n",
    "        R.c  = np.zeros(sizeHidden)\n",
    "        for i in range(40):\n",
    "            ret = R.inference(seed)\n",
    "            #print(i,\":\",ret)\n",
    "            result.append(ret)\n",
    "            seed = encode(ret,vocab_size)\n",
    "        decode = ''.join([ix_to_char[ch] for ch in result] )\n",
    "        print(decode)\n",
    "\n",
    "    position += seq_length;\n",
    "    n += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4]\n"
     ]
    }
   ],
   "source": [
    "k = [1,2,3,4]\n",
    "u = k[3:]\n",
    "print(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gnumpy as gpu\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
