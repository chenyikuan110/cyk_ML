{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431673 ,  79\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "\n",
    "data = open('HP1.txt','r', encoding=\"utf8\").read();\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(data_size,\", \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'-': 0, 'n': 1, 'Q': 2, '9': 3, 'M': 4, 'm': 5, 'k': 6, '?': 7, 'a': 8, '\\n': 9, 'R': 10, 't': 11, 'Y': 12, '(': 13, '!': 14, ')': 15, 'I': 16, '*': 17, '1': 18, '\\\\': 19, '6': 20, 'o': 21, 'E': 22, 'S': 23, '4': 24, 'c': 25, 'x': 26, 'q': 27, ' ': 28, 'b': 29, 'h': 30, 'U': 31, 'T': 32, 'K': 33, 'v': 34, 'N': 35, 'G': 36, 'X': 37, 'y': 38, 'g': 39, \"'\": 40, '7': 41, 'A': 42, 'Z': 43, 'W': 44, 'w': 45, 'd': 46, 'p': 47, 'F': 48, 'e': 49, '~': 50, 's': 51, '8': 52, 'D': 53, 'L': 54, ';': 55, 'V': 56, 'C': 57, 'j': 58, '\"': 59, 'i': 60, '\\t': 61, 'z': 62, 'O': 63, '3': 64, '0': 65, 'r': 66, 'P': 67, 'u': 68, ':': 69, 'f': 70, '5': 71, '.': 72, 'H': 73, 'B': 74, ',': 75, '2': 76, 'J': 77, 'l': 78}\n",
      "{0: '-', 1: 'n', 2: 'Q', 3: '9', 4: 'M', 5: 'm', 6: 'k', 7: '?', 8: 'a', 9: '\\n', 10: 'R', 11: 't', 12: 'Y', 13: '(', 14: '!', 15: ')', 16: 'I', 17: '*', 18: '1', 19: '\\\\', 20: '6', 21: 'o', 22: 'E', 23: 'S', 24: '4', 25: 'c', 26: 'x', 27: 'q', 28: ' ', 29: 'b', 30: 'h', 31: 'U', 32: 'T', 33: 'K', 34: 'v', 35: 'N', 36: 'G', 37: 'X', 38: 'y', 39: 'g', 40: \"'\", 41: '7', 42: 'A', 43: 'Z', 44: 'W', 45: 'w', 46: 'd', 47: 'p', 48: 'F', 49: 'e', 50: '~', 51: 's', 52: '8', 53: 'D', 54: 'L', 55: ';', 56: 'V', 57: 'C', 58: 'j', 59: '\"', 60: 'i', 61: '\\t', 62: 'z', 63: 'O', 64: '3', 65: '0', 66: 'r', 67: 'P', 68: 'u', 69: ':', 70: 'f', 71: '5', 72: '.', 73: 'H', 74: 'B', 75: ',', 76: '2', 77: 'J', 78: 'l'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of input chars & indices\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# demo of onehot encoding\n",
    "vector_for_char_a = np.zeros((vocab_size,1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "hidden_size = 100\n",
    "hidden_size2 =50\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "# the lower the learning rate, the quicker the network abandons old belief for new input\n",
    "# e.g. train images on dogs, give a cat, low learning rate will consider cat is anormally rather than dog\n",
    "\n",
    "# Model params\n",
    "Wxh1 = np.random.randn(hidden_size,  vocab_size)* 0.01 # input to hidden (input is onehot encoded)\n",
    "Wh1h2 = np.random.randn(hidden_size2, hidden_size)* 0.01 # hidden 1 to hidden 2\n",
    "Wh2h3 = np.random.randn(hidden_size, hidden_size2)* 0.01 # hidden 2 to hidden 3\n",
    "Wh3y = np.random.randn(vocab_size,  hidden_size)* 0.01 # hidden to output(decode the output)\n",
    "\n",
    "Wh1h1 = np.random.randn(hidden_size, hidden_size)* 0.01 # recurrent hidden .\n",
    "Wh2h2 = np.random.randn(hidden_size2, hidden_size2)* 0.01 # recurrent hidden .\n",
    "Wh3h3 = np.random.randn(hidden_size, hidden_size)* 0.01 # recurrent hidden .\n",
    "\n",
    "Bxh1 = np.zeros((hidden_size,1))\n",
    "Bh1h2 = np.zeros((hidden_size2,1))\n",
    "Bh2h3 = np.zeros((hidden_size,1))\n",
    "Bh3y = np.zeros((vocab_size,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax helper\n",
    "def softmax(seq):\n",
    "    return np.exp(seq)/ np.sum(np.exp(seq))\n",
    "\n",
    "def softmax_array(two_D_seq,t):\n",
    "    return np.exp(two_D_seq[t])/ np.sum(np.exp(two_D_seq[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function - training\n",
    "def lossFunction(inputs, targets, prev_hidden1, prev_hidden2, prev_hidden3):\n",
    "    # p is softmax probability\n",
    "    xs, h1s, h2s, h3s, ys, ps = {},{},{},{},{},{};\n",
    "    \n",
    "    h1s[-1] = copy.deepcopy(prev_hidden1)\n",
    "    h2s[-1] = copy.deepcopy(prev_hidden2)\n",
    "    h3s[-1] = copy.deepcopy(prev_hidden3)\n",
    "    loss = 0;\n",
    "    \n",
    "    # Fwd pass    \n",
    "    for t in range(len(inputs)):\n",
    "        # One hot encoding for the input char using our dictionary\n",
    "        xs[t] = np.zeros((vocab_size,1));\n",
    "        xs[t][inputs[t]] = 1; \n",
    "        \n",
    "        h1s[t] = np.tanh(np.dot(Wxh1,xs[t]) + np.dot(Wh1h1,h1s[t-1]) + Bxh1);\n",
    "        h2s[t] = np.tanh(np.dot(Wh1h2,h1s[t]) + np.dot(Wh2h2,h2s[t-1]) + Bh1h2);\n",
    "        h3s[t] = np.tanh(np.dot(Wh2h3,h2s[t]) + np.dot(Wh3h3,h3s[t-1]) + Bh2h3);\n",
    "        \n",
    "        ys[t] = np.dot(Wh3y,h3s[t]) + Bh3y;\n",
    "        ps[t] = softmax(ys[t]);\n",
    "        char_idx = targets[t]\n",
    "        loss += -np.log(ps[t][char_idx,0]) \n",
    "        # ps[t][targets[t]] is the prob. node corrs. to. t_th char in the label array\n",
    "\n",
    "        \n",
    "    # Gradient value holders\n",
    "    dWxh1, dWh1h2, dWh2h3, dWh3y = np.zeros_like(Wxh1 ),np.zeros_like(Wh1h2),np.zeros_like(Wh2h3),np.zeros_like(Wh2y)\n",
    "    dWh1h1,dWh2h2, dWh3h3        = np.zeros_like(Wh1h1),np.zeros_like(Wh2h2),np.zeros_like(Wh3h3)\n",
    "    dBxh1, dBh1h2, dBh2h3, dBh3y = np.zeros_like(Bxh1 ),np.zeros_like(Bh1h2),np.zeros_like(Bh2h3),np.zeros_like(Bh3y)\n",
    "    dh1next = np.zeros_like(h1s[0])\n",
    "    dh2next = np.zeros_like(h2s[0])\n",
    "    dh3next = np.zeros_like(h3s[0])\n",
    "    \n",
    "    # Bwd pass\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        \n",
    "        # prob. p to output y\n",
    "        dy = copy.deepcopy(ps[t])\n",
    "        dy[targets[t]] -= 1 # this is how we calculate loss using onehot encoding\n",
    "        \n",
    "        # y to h3 \n",
    "        dWh3y += np.dot(dy, h3s[t].T);        \n",
    "        dBh3y += dy # derivative w.r.t bias is 1        \n",
    "        dh3 = np.dot(Wh3y.T,dy) + dh3next # back prop the error from y into h3\n",
    "        \n",
    "        # h3 to h2 and h3_prev\n",
    "        dh3raw = (1-h3s[t]*h3s[t])*dh3 # back prop thru tanh\n",
    "        dBh2h3 += dh3raw  # derivative of Wx+b w.r.t b is 1; d_loss/d_b = d_loss/d_H * d_H/d_b\n",
    "        dWh2h3 += np.dot(dh3raw, h2s[t].T) # derivative of Wx+b w.r.t W is x\n",
    "        dWh3h3 += np.dot(dh3raw,h3s[t-1].T)\n",
    "        dh3next = np.dot(Wh3h3.T, dh3raw)\n",
    "        \n",
    "        dh2 = np.dot(Wh2h3.T,dh3) + dh2next # back prop the error from h3 into h2     \n",
    "        \n",
    "        # h2 to h1 and h2_prev\n",
    "        dh2raw = (1-h2s[t]*h2s[t])*dh2 # back prop thru tanh\n",
    "        dBh1h2 += dh2raw  # derivative of Wx+b w.r.t b is 1; d_loss/d_b = d_loss/d_H * d_H/d_b\n",
    "        dWh1h2 += np.dot(dh2raw, h1s[t].T) # derivative of Wx+b w.r.t W is x\n",
    "        dWh2h2 += np.dot(dh2raw,h2s[t-1].T)\n",
    "        dh2next = np.dot(Wh2h2.T, dh2raw)\n",
    "        \n",
    "        dh1 = np.dot(Wh1h2.T,dh2) + dh1next # back prop the error from y into h2\n",
    "        \n",
    "        # h1 to x and h1_prev\n",
    "        dh1raw = (1-h1s[t]*h1s[t])*dh1 # back prop thru tanh        \n",
    "        dBxh1 += dh1raw  # derivative of Wx+b w.r.t b is 1; d_loss/d_b = d_loss/d_H * d_H/d_b\n",
    "        dWxh1 += np.dot(dh1raw, xs[t].T) # derivative of Wx+b w.r.t W is x\n",
    "        dWh1h1 += np.dot(dh1raw,h1s[t-1].T)\n",
    "        dh1next = np.dot(Wh1h1.T, dh1raw)\n",
    "\n",
    "    # Can be replaced using LSTM structure\n",
    "    for dparam in [dWxh1, dWh1h1, dWh1h2, dWh2h2, dWh2h3, dWh3h3, dWh3y, dBxh1, dBh1h2, dBh2h3,dBh3y]:\n",
    "        np.clip(dparam,-5,5,out=dparam) # mitigate gradient vanish\n",
    "        \n",
    "        \n",
    "    return loss,dWxh1,dWh1h2,dWh1h1,dWh2h3,dWh2h2,dWh3y,dWh3h3,dBxh1,dBh1h2,dBh2h3,dBh3y,h1s[len(inputs)-1],h2s[len(inputs)-1],h3s[len(inputs)-1];\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " CjKEYN.xa-qZwYtYSt~;PlKrXo(TB?-(ds4\\?e?B:mY\\ Ngupsyr(FMZIhOJK.kt4MKP4Mw,0Em.bJYL\"JgmQ(xQ\tY)ofrsV~rGJDir\t4-YP;a7o7(5Eh\t2cr.1oT JPN)kFPMIRs5a3Y~2ng!\n",
      "JuQ)u9)Rl,ewksE4w1sp2Z!-Y9Zyd\"CFa0)TFwmKdkNrHLkK?1\\5d \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "def sample(h1,h2,h3,seed_ix,n):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # one hot encode\n",
    "    x[seed_ix] = 1;\n",
    "    ixes = [] # empty sentence\n",
    "\n",
    "    for t in range(n):\n",
    "\n",
    "        h1 = np.tanh(np.dot(Wxh1 ,x ) + np.dot(Wh1h1,h1) + Bxh1 );\n",
    "        h2 = np.tanh(np.dot(Wh1h2,h1) + np.dot(Wh2h2,h2) + Bh1h2);\n",
    "        h3 = np.tanh(np.dot(Wh2h3,h2) + np.dot(Wh3h3,h3) + Bh2h3);\n",
    "        y = np.dot(Wh3y,h3) + Bh3y;\n",
    "        p = softmax(y);\n",
    "\n",
    "        # sample the output\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        # encode this output\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[ix] = 1;\n",
    "\n",
    "        ixes.append(ix)\n",
    "        # if n > 1, it will predict more than 1 subsequent chars\n",
    "        \n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print (\"----\\n %s \\n----\" % (txt,))\n",
    "    \n",
    "# test\n",
    "h1prev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "h2prev = np.zeros((hidden_size2,1)) # reset RNN memory  \n",
    "h3prev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(h1prev,h2prev,h3prev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorc\n",
      "inputs [73, 8, 66, 66, 38, 28, 67, 21, 11, 11, 49, 66, 28, 8, 1, 46, 28, 11, 30, 49, 28, 23, 21, 66, 25]\n",
      "arry Potter and the Sorce\n",
      "targets [8, 66, 66, 38, 28, 67, 21, 11, 11, 49, 66, 28, 8, 1, 46, 28, 11, 30, 49, 28, 23, 21, 66, 25, 49]\n"
     ]
    }
   ],
   "source": [
    "# Training using Adagrad (decreasing learning rate)\n",
    "position = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "print(data[position:position+seq_length])\n",
    "print(\"inputs\",inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "print(data[position+1:position+seq_length+1])\n",
    "print(\"targets\",targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 109.236196265\n",
      "----\n",
      " -UGm'i-eT8P.L04I.U-;xEGLJS\"AoFQkbt*U;aGrrtIADHxbr5j5\"NBG-CCmS.RSVZ9:-97jhP3\\VpPnyPs2-Fdhf49JtCA'z\\r,:1eBiLu~u(*P1c?sHzWqf1PQdUpIP;x\"679:OGPbQFa~kmlZ93*hDXefM'4ssM\\9Wu,P8s\n",
      "L*(N4o,9mQ)IZI')ZnRe;~5!h6\n",
      "03 \n",
      "----\n",
      "1000 loss 93.3072365399\n",
      "----\n",
      " f   v  dmsasa etsioleilt t ooioemOl rr.tnkee  os wl'ertsa,t ouyosklte\n",
      "eheoe sn,y slh uf\"csaohgd ksue eiaaltn erssbstsi?g ros  bvaaroei uamd s wvrtgier  dieae.yaadt e\"gneweu iiw\n",
      "yy  clgself lrnnida  ic \n",
      "----\n",
      "2000 loss 84.1486283351\n",
      "----\n",
      " ateihende,ate csnhe knd ro slitl.edeo.rcnhrynNedooe nhdire bhnelwe giDntehake. ntera. tbsee augandl .h bpita'e twtiaours\n",
      "\n",
      "ilh ulck.eosd Bwdgor thg hnteelzobiortodes!A  ib dr hhtH atA iane. l edeeoerr  \n",
      "----\n",
      "3000 loss 76.7837769842\n",
      "----\n",
      " ediw lhl oo' tHeeesifae brnl\n",
      "ifke\n",
      "\"\"\"\n",
      "WEwt-iheeg aeegtp lok bay poeu,. wafeiwcslt, g\".Tdey\n",
      "  -visbr semsh kerp k hs who grmerwer.ie At. \"anrotide bn. 8ons dema.ee dugs likr curtod wherd  4utk,, Oner b \n",
      "----\n",
      "4000 loss 72.8130407837\n",
      "----\n",
      " poiugocke, dan jtcEas ocfow, pntokn bgs-aneiakrtl oflmonl sllm thnrd Psjgcmigd -Aorrtdh\n",
      ")enimhe whrld pgireuz Birdeslr heyeiheon, Hay slat nagr Uoniud tamr c dheeint orrm sam tav sheehert iottsinaoteu \n",
      "----\n",
      "5000 loss 69.4223968384\n",
      "----\n",
      " agd hor'tk soreey onn m'Iif oot ben sh haw?\"\"\"Sast\n",
      "\"a sent hes.\n",
      "Aaoik..\"P wheey iobefWoiut labrhrt he nnim. Hur ar nonacpmFsd.t\n",
      "\"Yr blyaw.\" auney, ig thes vep nnt otee hocey\n",
      "H\"\n",
      "H?\"\"Iaran unchu tate.ee \n",
      "----\n",
      "6000 loss 67.0731791204\n",
      "----\n",
      " hen Cee.. Iy onasdr t-v. Hald sotheod ond myet rdaocga hak staast homhe cod lirestayit sag they\n",
      "\"Onolard hefdee. iu shehs sothe. Yuswh tean. Gn Hheawtrt \"hrd b'y clideapy'oyis thr'od'w\"\"\n",
      "\"\"(Dyin voc.\" \n",
      "----\n",
      "7000 loss 66.1151643061\n",
      "----\n",
      "  cdonk ducberr hasent theeyd pgowecpiney,eey ee lnaud a Beas tarrrd dume doo lasg h. Heais Sher un apof, Ha leanle,\" waa rrmuzt ded Hin tios was logeat Gehy if wid hom le Ralyy tiy wa d soed H lagr da \n",
      "----\n",
      "8000 loss 65.9403867105\n",
      "----\n",
      " uston athe wiood fom,hig, wi orlast halar hlinustenl winrshf npcgednrndrandlyi gatdos Hadayrrid at sa hotiog daryonidlos aapts wheggareye on dvaum:. Rant fr?eenyticy, upmta'tt.\n",
      "\"Anrd notn iloch whir g \n",
      "----\n",
      "9000 loss 65.3672214938\n",
      "----\n",
      "  wurdlnvrt \"-h weet Comfos v'vee teerm?\"\"Siseysriny anore' sundhimsirry oisle ?her nund hov. aiue Hisle dontt hathhet, gtheilr Heioginn nore. Toegu pto ve lrethrs bhang .\n",
      "Neird Foug.\n",
      "\"Soy Ht i rlle an \n",
      "----\n",
      "10000 loss 63.9092583354\n",
      "----\n",
      " eet jalaid van fogl cisend efee toarr hicnim lam Regs ur! son wir.\"\n",
      "\"\"-?\n",
      "\"They on afe av they Hat olacked dalm. Ash srilk, sr he heil aof gev mou, b-  ea, hln of the hal. Mre got.'s areet,ad, the\"\" Sa \n",
      "----\n",
      "11000 loss 63.6687058675\n",
      "----\n",
      "  -o vsotran- aiph gioc, Har jpoigisrs yol himire t-iuneleY yarisd iron qnir Had eure -e Hug'tte hog Har, enrltougseoots\n",
      "\"h Ha gller'l.\"\n",
      "Hatta keap. \"Ir'pang tur \n",
      "ieey.rag, hay lonop to, ft fer the of, \n",
      "----\n",
      "12000 loss 62.0669520842\n",
      "----\n",
      " cor hint, mozh boslr as on Fhes oe araug'me lit mad ff ronne'n, inoe v, hordiweoswad deld goinw toc weat ok thr his wiriv heit. HE ver t ol made. \"TIen tav ofuwl.or sw murevasold thakr coab hatrad dae \n",
      "----\n",
      "13000 loss 62.0393450599\n",
      "----\n",
      " ndos.\"\n",
      "\"I theoreiy rilreingat peaded. \"Weay hirr,. Unler.\"\n",
      "I tee ne hakonl wadee nr'the,\n",
      "\"Arteos' teteth -iniuw. \"Eeoukeing friad.\"\n",
      "\"Nnnfavlboreed heag, Feribk fneosdeit.\n",
      "\"Ynad marr the awy y corling  \n",
      "----\n",
      "14000 loss 61.4131761882\n",
      "----\n",
      " osy.\n",
      "Tlatee'dt\" Nladt iubt kary the ont,\n",
      "\"Bome puw ofey, Ioakey they ol t-\n",
      "algeanl ot masde soe, iuce luwke soreion ings por, oson heyt thm af hats -hau co'rtearl thrr hip piyt.\n",
      "\"\"ane t-\n",
      "\"TWis waste b \n",
      "----\n",
      "15000 loss 61.3280027545\n",
      "----\n",
      " reing it Ioy oreafy biod.\"\n",
      "Hand the'sgon lecmad is clrlnm'f oniy,hny Raid ludhiope.\"\"\" H- pelnt'!\n",
      "\"I'qe yan  ampitlyozer came Ioch.\"\n",
      "Heom Feyink nale, --ene a firit honb to hem Fa mad moukid Cradsit\"  \n",
      "----\n",
      "16000 loss 59.887147454\n",
      "----\n",
      "  tos deps, and I ofny ppaik uach th ons them! Iin duun the counters she poun, wot Ianan to weerd deok ot\"Hark Slanr a thif fhe orlingd at hace wireyangh, the cee and,\n",
      "\"Eo cattere ha weid mast,\n",
      "\"Whe dv \n",
      "----\n",
      "17000 loss 60.1024069705\n",
      "----\n",
      " n goye hait, ost af reto mal rinveored Hed Hat tished hav's -ody. \"Derny that ghel's and Leolnrlde sa adp iits molrey afn om wakeiriteale be and the Histetale'd toreiore'ly. Anr -- the wate ue sretell \n",
      "----\n",
      "18000 loss 59.767638936\n",
      "----\n",
      " f ront. Eos wecch i-! Bremlt?'t;hot...out, iun'pe staey Duge'lting.\n",
      "\"Durl'g atmole. Aut has brett'ory'r bkone ures waed oore wer, atam Hem Pntarsa toodid fin werrerlis hore. Jo fameynin. Oc\"Wonthed he \n",
      "----\n",
      "19000 loss 59.2107338168\n",
      "----\n",
      " i. the weu'weave him Us anrlone vesh n radf saare,. He ceepid a daels.\" wilc\"\n",
      "YAUnr'n tha dinl..\n",
      "\"Gnmeys,\"'d thios asdern, ceanlopiv sxore robmoting hip cory tag hars. ubreiny herd mamessyeik. Burf, I \n",
      "----\n",
      "20000 loss 58.5930423117\n",
      "----\n",
      " heg ip furn hisbld lare mipke said bren hoik itKt hameen.\n",
      "\"Aeem's ameuneotot  ar root hick it the hab the womepf swons uf the more apreyy si phaodcpeyon ashed fery biuk then the garid weerrs oud?\"\"\"\n",
      "I \n",
      "----\n",
      "21000 loss 60.5713212364\n",
      "----\n",
      " he ataze.\n",
      "\"Et\n",
      "\"Caufto chamde gut epslats bo hit nit Joagting Hog Anho.\" srecmerleare stosenw.'ly Ire ensoze thern, He tageing Mas, Unint Ieay ipe,flbsshan wagmernd broftd Bt\n",
      "SYGudined Hatcey wans mrL\" \n",
      "----\n",
      "22000 loss 59.7837112312\n",
      "----\n",
      " y nidildon wich the not, hind the neg. Iuile me cale't fr'f,\n",
      "Sre tearey. \"Yeot ared bo sad Hasem badqtoer wovee torendsdis wos rerlooks the yle.\n",
      "\"Aots aubead, encaviled. Aoar the billying rire' staom  \n",
      "----\n",
      "23000 loss 58.7437770224\n",
      "----\n",
      " he -hirs awh reerus Roif- Haer, ox thqos it.\"\n",
      "Hiplivn of thug thaly uramany?\" saky Poudfid um ncI nas soceavef poll and Ir han. Ianyiw boveld,et.\" Shr rimtt\"\"shorade Caesevan bifass. Oe'ne was peet wo \n",
      "----\n",
      "24000 loss 58.648070659\n",
      "----\n",
      " rged the his Ropes Futhon jattmoded, walh lerel.\n",
      "Aw Jrreli'w!\n",
      "\"P'd i he's Me,il\n",
      "\"Harry,, aftelired, Alledd Halry , Ha themlld of the harredcad leut he douilots's ygand twalked Pothery shipe Cleres.\"\n",
      "o \n",
      "----\n",
      "25000 loss 58.3785985126\n",
      "----\n",
      "  Ho drag all hos ofosl ar.\n",
      "Hagtwoy the fecape Adevhee ily ifiar-whor Prereon's and bolnss blok ot uritw qomime onuugse'wtwarse to the yed yle dean dat -hlag Hat and all try hag wotf. Sus seow- sheC,'s \n",
      "----\n",
      "26000 loss 57.8045179621\n",
      "----\n",
      " e R tame wol Madsoon un hig teose Worln,ow, anthind aso wast,, browe?\"\n",
      "\"Yo d sf hisses. \"N kat it\" wavougl.\"\" the barpouding oftwardee ot cin. Pood fhryrvne sry Gecay woulgeronor w- ih hl twot son epu \n",
      "----\n",
      "27000 loss 57.4599304177\n",
      "----\n",
      " zwerrd roll -he end sean'r gcaundlos, fosfungo insi knoched.\n",
      "Whe brothing fot sore dool bavepeand thein of her that to go goped mot bagtith.\n",
      "\n",
      "I yrikf. \"N he mhagst tharly -- Rostte utaushon um the a k \n",
      "----\n",
      "28000 loss 56.800432545\n",
      "----\n",
      " en me ft hinglt lacslele shotp. \"Stk -- con Ha penlm e saddind iot it -h t-e w-Stst bome iphind Mliuts -- necoe teanshud enth he ond\" mad'rot inches they my cud-he fay caime. Rames.\n",
      "\"Drevn thandbhabri \n",
      "----\n",
      "29000 loss 56.250835098\n",
      "----\n",
      " e stering, she nrist ou slot rans wind at to, houm Mremiuk he bd enidson't.\" y andef the, thewher and firesing d so shagln and foth and hagped. Harry saary himlby rukf y the lorlis shect tay Front ot  \n",
      "----\n",
      "30000 loss 56.3656636598\n",
      "----\n",
      " re, he andeure tapb emasf. I an nt thee a erory el?\n",
      "\"I he sat donrsice tucetizit. Wherectelad ghr gaigs neyer padk and. \"Choun sourly beridepf. He Leipld saire cuopey thirchery awesen. \"-- He dole thi \n",
      "----\n",
      "31000 loss 56.1958912264\n",
      "----\n",
      " t tose.\n",
      "Harly lvidisping.\" mane,\"\n",
      "\"Yole. Wurith mas tog thest.\" Peare dingion, juredtevy cuvped roudek yamyer. \"This to soulped.\"\n",
      "Hittke sth was Foar inhir weirtchang that Hadargy'arle sa wirlang hxwe \n",
      "----\n",
      "32000 loss 56.2012480072\n",
      "----\n",
      " rgs, wareman,sr. V the Haad, ovlses bale a fee foud qeyt naided stsidf af don ogher to bouke hermiog- doit that Pnit beuhe wolked siredtastet onn't!\n",
      "\"Eo cawithing, sroger yad Nedessile tist bundlenf.\" \n",
      "----\n",
      "33000 loss 55.448748496\n",
      "----\n",
      " ickoryotry fpol themp feas?\" I ale her y'iter on The tee to ceile werids tilkes 3ut them! sasr wa cIes. Hord anu perm ithingor.\n",
      "Anat, bind pustfeved that to nuun,. \"The kugk hi miwedederd dowtt buveri \n",
      "----\n",
      "34000 loss 55.3504928701\n",
      "----\n",
      " liin that itee,,haol, he alent there to the, the. Soury weids ule lot chire was i and  out. \"he seytseyenh thodt. \"Ylarilkiot and.\"\"\"\", as madtor en unweroileten,.\". Vooted qrly fowberid aboiust --\n",
      "Ve \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000 loss 55.7012293707\n",
      "----\n",
      " divbe I soreilas sfeanpsiut!. Jare a nrething was srln anas. Ity as thuse changos ,as salcege sowtt aved in ther bigter.\".oerantle (ussoothroit bo mo toieo or nn anen and am, the ne uulfaed axrand the \n",
      "----\n",
      "36000 loss 55.5989015297\n",
      "----\n",
      "  Wh thl hapriss hustar theyl hewone mere a bakting the Veod oully Panesad hefs bolg curets?\"\n",
      "Harrahot throng vedts't of abchey,\"\n",
      "\"Nook in tAore. errine hadd spot?\" s narbe the minn yiseechos you his't \n",
      "----\n",
      "37000 loss 55.1043316111\n",
      "----\n",
      " hey asten of stas Iae laclen. He boswach bagcing wat gayay at the Velring herand vaskey mereed they cunk on, hetcoud to Dumt hamt lo kfeount-ert at the hertin the knst and abarreit and uldoutod\" gard. \n",
      "----\n",
      "38000 loss 56.3163380853\n",
      "----\n",
      " po tha cae dad't roocher Pat. I pile't ot nome' ob ine. A betoileg uudsing chYtkent iot choot thees Puniththe teikey ens ontoct yeo his lorssly or har gighunditn fmade somer's jeagh at erounde. tha mi \n",
      "----\n",
      "39000 loss 56.2988414396\n",
      "----\n",
      "  a guinbed a rlan a welly eve't the keug deell, yowes their weach eg fedrns of fou fregoud the meep ox pis horedore stoole, His you uby rourse apbarge thet to wo thits meage was.\"\"\n",
      "Hard'rind but teedo \n",
      "----\n",
      "40000 loss 55.4884736642\n",
      "----\n",
      " ed on, throiss fooky sere'h bam'tts-oted thed sarlins the ewchis dett'r,. Harry, Theudy -- at of -hcthh youry was and gorcand in the bilyed st posney, had aide?\" Harry yglyeylas. Aaland to ktre the st \n",
      "----\n",
      "41000 loss 55.1705855329\n",
      "----\n",
      "  oinad ragtedaare barcepos. It, oe the kniled\n",
      "I she ah Ereates,\"\n",
      "Harry woodind m thifs io had owalared the weed the he and peadle Put otttr parked heard,ecurr looktinn gerets wutk (ires ved thime weth \n",
      "----\n",
      "42000 loss 55.5494966954\n",
      "----\n",
      " d twe StoR hadonolessere lindtming intf souviygefsta Elaise siek, thers bnar if bonn pray jant. \"Anw-the celfiis as tagcane rolyss\" Has. Pnxlaking to rize the cal enr on's lnele?\"\n",
      "\"Bot hy pueswey, in  \n",
      "----\n",
      "43000 loss 55.0797302077\n",
      "----\n",
      "  the hacfsc, he liidads \"-Aod --\n",
      "Hadt, he bryst deafed axh Mudningoramed Iteridlaring, you wacthoor fot the peas,\" ufftuwumok apering he this no stog noime ar of Mund ape wis dou cile un os,\" ll whivs \n",
      "----\n",
      "44000 loss 54.4989552894\n",
      "----\n",
      " eekylidn's has was a ke biidgteir thane misf on lere thering, ass. \"Bon unt arrey alle wesf\"\"\"\"\"\"\"Sthas yound rop,. Tu diog abmefey thos -- kelirls was they. Harry and Peids tu hes proukds.\"\n",
      "\"Ther to  \n",
      "----\n",
      "45000 loss 54.2650714783\n",
      "----\n",
      " ngher stiin agh pe taltingow\"\n",
      "Hexd, Sf. Sg Jod. Wooy Rheh themedd elofpe loth\" Cume't thave unde's I smft betan, stpenn becore Meangelytfir'\" camelvG wiplat, eded rod --' fad arrinal pith Isits one to \n",
      "----\n",
      "46000 loss 54.3517501472\n",
      "----\n",
      " on HI's clered abcook Woll pere in the peokad allound and wol yiul fore and the wamed tow\"\n",
      "Eot thoope,.\"\n",
      "Hap in was tos feops an arly thenter go olden hive raring op.\"\n",
      "Bufe eir, of heM your picfeetard \n",
      "----\n",
      "47000 loss 53.676235487\n",
      "----\n",
      " e's to in hime't whe's save het Loastete olide, unaarly Hh. Noitting heany yearpe Ropos fnefered see dely eeding, hey\"et of soepour.\" \"oniowed peer so sis and Oevubmemerpicttfint in the Sthy I thigh h \n",
      "----\n",
      "48000 loss 53.6844427792\n",
      "----\n",
      " pso Sneadthe, innt to dreff thid,ly che. Thigkon chat Hith, he workd ceamig.\n",
      "The soy Fest -hery an and Nound bvibunus ta ghuntey.\n",
      "\"on ify thed havy and How of Rome thowit doufed hastid Me'n moin of th \n",
      "----\n",
      "49000 loss 53.6599851671\n",
      "----\n",
      " haicews whois --ing the beand was ophid amalle and then, I tan's, hathen oureinp.\"Han wile but, hoothiy roythy had hay cor angore, w- ath the leingingbinded then it. Heened att un, swoifer a hooge loo \n",
      "----\n",
      "50000 loss 53.3972672644\n",
      "----\n",
      " oss. She svaen?\" Neis cich of. \"\"hr't krostor, leesly to gon aweg lren it this auzhingwert aen rired faetaive ceouninghores, stunk,,\" is one.. An'k intisnes boereds inn's Gore'r athed. He feen,, Natt. \n",
      "----\n",
      "51000 loss 53.1874567004\n",
      "----\n",
      "  as buvuth,hrod wy what,He fulls milhind foke he inn ar fere'n.\n",
      "Is Grild and blall. \"Une im it prot dout then. Qcinne floore heins enche!\n",
      "Harr?hered chow. Has, yaw to ping'n! HI mhoreher se tras wasre \n",
      "----\n",
      "52000 loss 53.7010883005\n",
      "----\n",
      " and!er ale has sopvyn's on and. Souged peaghicong paradlor. They gracuify wace the you an isoolw, last atmyHhining beter hennd beilly sledegon to mf at Grwlirrore with thetheim, torden thefed op them  \n",
      "----\n",
      "53000 loss 53.2006713913\n",
      "----\n",
      " ing oning cmiflinglrfs tine oey is eore ans; he if thep we meilf. He loed he. I bary had malk. Hogparde, nosstheas delf wear. \"Rof yoar or a sim.\n",
      "He lideds,arding tha sqogetis Durkerecosrs of anetllet \n",
      "----\n",
      "54000 loss 53.1948054938\n",
      "----\n",
      "  stbben? Deipdrall tmas wit there bigche hearny, ous otess on the dory inbar toll oy Ve. The filicl was arceflad. Need stound botlitt weeladed Put. Dunled fTkwert. Dudleyla goudarey, wopred buthel fo  \n",
      "----\n",
      "55000 loss 54.0743682719\n",
      "----\n",
      " at. WwI, Vrouge Harryd'r the dided was decrinen't oee so magit enomd in techer Haed a gurrthid blook intlen tarem lile ome tth the but in hid. And lherd'dfeird Mumnfryed to lo gouldnen, ttebiue seiwte \n",
      "----\n",
      "56000 loss 54.8684084993\n",
      "----\n",
      " lios. Icwall thing\n",
      "Harly, bapg, st't tro parmey. En wan ilalr, knoh tame anuge his peogh them, addrres.\n",
      "Him.:oeden to did you dad ludchime inw ou dooke rore to hern of rintoxkror the beechiplinom self \n",
      "----\n",
      "57000 loss 54.1190138126\n",
      "----\n",
      " ur. Hagry yel gosnld and no gh I's it itaed hagrn't me if.. Hit,ore.\n",
      "\"Nnpf palnotee; the. \"The, istos. Hatre't tu denag ttuch they bat in tuy vy sarinnoo dirred fnis a derin,, he it but ourd) wis a vh \n",
      "----\n",
      "58000 loss 53.3888705793\n",
      "----\n",
      " sving louis.\n",
      "\"He boversing.\n",
      "\"Ther twe leend inte herey avkien, it awyed sttling secaed all. Reytot soey thoimps. Deich the s ave rond hithen Rucklts, Harry?\n",
      "I didirtelpoy and of hive tuchto wame swhen \n",
      "----\n",
      "59000 loss 53.8238146871\n",
      "----\n",
      " ap time insay, Iber,\n",
      "he him's was all ttilled all a he ofer Lowe tepe shom the aghor, you sold,?\" said.\n",
      "\n",
      "GRLORUteoll aed shouks ta Paghding trew and hepee, Reytress yokdelser. There Sna kle fy hid aww \n",
      "----\n",
      "60000 loss 53.3336600106\n",
      "----\n",
      " e thefomy,, Preled thachralid gIt sttaite wad chov Gracht yive hid yiy it had inwe'dbod on, a the cheirgome,\" lagis weargid at of thip the, ceoirglicled unmor, Kp one 'reurawed im that Pottille. thees \n",
      "----\n",
      "61000 loss 52.8312721489\n",
      "----\n",
      " tGrestorsn't Itwann,\" seread'n the chaedetibtw and Nadd!\n",
      "\"Larrire as fnem, wathingids\n",
      "\"Or.\n",
      "\"Sn\"\n",
      "Ha ben onipe ly a pit vew ol sw unle out fack ou heach to hid Rrot besfildiyflin buttray dramed gall cug \n",
      "----\n",
      "62000 loss 52.0564259597\n",
      "----\n",
      " cs oot roine as the lides foindtomartt rlalils'ne me, Hamd to we hewhan. Blatcined the from feoping EpGle. Her fore wouch Geymind feob sort,, ytt a pan peales arle treyorith wropes ofe the dred sthid, \n",
      "----\n",
      "63000 loss 52.5430485468\n",
      "----\n",
      " lowy womy the lickelnesed and peand whipilir \"hat!\n",
      "Qo flatginss the nogeey. Eut an't and twhew or the nad, non. \"So teetad ot bookpee't  of con her willnd'g hog thvy Qolgess -- meed againe lideray.\n",
      "\"I \n",
      "----\n",
      "64000 loss 51.7724666937\n",
      "----\n",
      " 's\".\n",
      "\"A lopeter y) Fufving thinks corere of cHber broghtey Gtt Iey dedmoutbonp and a loumet rouded alle jut, he -arkedsHand wafe, Flidald black rofved unst\" Noutin's had dassuold balled, I\n",
      "Tht feakn t \n",
      "----\n",
      "65000 loss 52.0731772009\n",
      "----\n",
      "  lall'n't ov turly..\n",
      "The waincer..\n",
      "\"ovarim, of sam witot's Hamey halun. He'med'uvly weg cuelb,.\n",
      "\"That?\" \"on's tating sund at o4e three it henriss it taicuzes, wothDr! \"Yors. T-AI\n",
      "\"Hyr'ps Crelt of a ha \n",
      "----\n",
      "66000 loss 52.1956569426\n",
      "----\n",
      " rboled, as ovat aact -- W-w.\n",
      "\"RGund Harry put turing youtoine't cungorery.\"\n",
      "Harry hove fowd sthe birdiand madyor,\"\"W\"\n",
      "Wad on his,\" say creround, I gomy haythithiery. Harry hisgewis abaaring. Waed.\n",
      "\"En \n",
      "----\n",
      "67000 loss 51.7739688126\n",
      "----\n",
      "  hamay,\" said Harry than aoh yit was witauld lirly foch thisolibes how there scroyn evoumed beekarf hit on the peeland uds ie. \"Runh to be uramad mizstin them to Gowe lindhy reds glavurd.\n",
      "\"Drife. de,  \n",
      "----\n",
      "68000 loss 51.3381978012\n",
      "----\n",
      " th an!\n",
      "\"An ther, ere they down'ny hay Mudele, say girehp ne teas It i' ye werdy twere. \"\"Tt wit meen shisbed farker ule inf coldor unthey alo tuu poinvond il tuck rowe. \"ene fudt yours.\n",
      "\"Whaolly dail. \n",
      "----\n",
      "69000 loss 52.2449606351\n",
      "----\n",
      " The waig- woor.\n",
      "\"Wo cone!ry Harry love Than -- the wabpevaclong. \"An in a gethers.\"\n",
      "\"I voveds,\"ery leen tare the dtEuttd feethe bindtham thouge out poek ifstbing, enxh and on walfend them re show tcee \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000 loss 51.9314428757\n",
      "----\n",
      " treand of the bully.\" SpI very wistle.\n",
      "\"At Madk Harc frster,\" said Verling at whe Derond and cundor, veak. It limh,\"\n",
      "He sulvirne four,.\" sadamesmes in throw Must fighto, at ovebe, agofibl gon her a po \n",
      "----\n",
      "71000 loss 51.9107020875\n",
      "----\n",
      " ed. The stalled, them the mafly fin ofe ing, in betinal clakann to hid fabklmusor.\" sheyghr ckexnind so the cirdtador his meametsly dar, Po clire. Harrytpestnn out and thougvle.\" Habry dayn be shot of \n",
      "----\n",
      "72000 loss 52.376778663\n",
      "----\n",
      " d\"Ha walpsherbed,\"t't stateery you bhlsore all dotfee an favsory I srey sromer on yid shanin- magrining had bast tor a sover a throus gat bith bouisury fuener,\" hered lutle santher in the beecery wead \n",
      "----\n",
      "73000 loss 53.7005915103\n",
      "----\n",
      " d.\"\"\n",
      "\"LI''d him at hey sropes this Hacrionsent uvering toidinge. Ther dweg ose.....\"\n",
      "\"Sules an sosn, heom, asfer you,\" a pip bhes staln your Migdt allion.\n",
      "Whe SfGnand and seal belbpoun though bully fe \n",
      "----\n",
      "74000 loss 53.2616248323\n",
      "----\n",
      " leshad ttwoicuroll srang to who he tu tafhiling albearlplak.\n",
      "Hiply watsoary thee the gitt chat't\" Harry with a relary smect.\n",
      "It down domdwageroufs alber. He was n't, weringsoaulder panven was attoup e \n",
      "----\n",
      "75000 loss 52.261831907\n",
      "----\n",
      " to verr's someware therp-abt to melringin hibe trom sceamin' anr a melt kcoweroo car a plash,\" saud. Bettercbere sook ablabt petped mid lure't Pear ner ons hood soy a, sovershins on Auds.\"\n",
      "\"What wathe \n",
      "----\n",
      "76000 loss 52.4107731405\n",
      "----\n",
      " d sigh I. Ararsed hundery he and rookengaly shomet o, able hisncot Ion cuss whist qoughed in as Rraadden Mitcleiding at here you.\n",
      "\"I'm sreack hake,\n",
      "\"At; fere.\n",
      "Oees you suad, have pupled as piiteps por \n",
      "----\n",
      "77000 loss 52.1427487966\n",
      "----\n",
      " fe!\n",
      "\"Oon dold.\n",
      "Ptealy's seecearry lowrygere wil,. It pealled Harry horry, if thear.\n",
      "\"Anle so fece it his plise. \"Sonarise.\"\n",
      "Harry lipcosorhil every wealPered.\n",
      "\"He pragk Cithre so ikss toketh andhats s \n",
      "----\n",
      "78000 loss 51.5926845338\n",
      "----\n",
      " nver,\" ceinios a knoy, she Prwon anafipes. Whet laraeg wond soutto,er it urall modethy was thas. tokes. I cust Fait lold to the -hyache, you talben, a eyy out txasligchett os be Has baiged WitDon enic \n",
      "----\n",
      "79000 loss 50.9418612006\n",
      "----\n",
      " alcesnote coud on the foufe lowe or!\"\n",
      "En his fryat. He slipple. Rer hack.\n",
      "\"Buts ous weas wind and beon out if taunt oe trrit the giy goches ofhewarlised, stfing me fome thamforly PosKed os tliise when \n",
      "----\n",
      "80000 loss 51.3172843005\n",
      "----\n",
      " oosels, bround call. \"Harryuble, Lolreytor.\n",
      "\"Jmitth. \"Mof ssmily,\" shat't had to looh Fudle on thing,.\"en at it raycloly duch. Hat fores. Mi,t giminges, chetoaround kfad's,\" fere broack. He seach, you \n",
      "----\n",
      "81000 loss 50.4669960105\n",
      "----\n",
      " ickipute mound boisttorast the birbt it ant Ste.\" she rook birethrre hus bouvs nreathing -\" ilt ono a fiith'rs hikr as whatpens. We her Masted. \"Soe want'n wirided at luck. He dedt? Ham fude trother a \n",
      "----\n",
      "82000 loss 51.1075703324\n",
      "----\n",
      " ek luwh I'serinalins gtbhorle luyter untt, what fimisbe at lriles in Hhered, ytr are from she teasDs had we sculeding is got he and stteased.\n",
      "Ervat thofis tref's the Sleet, stallingt nar we Hit ceadge \n",
      "----\n",
      "83000 loss 50.7934218533\n",
      "----\n",
      " d.\n",
      "\"I w-racweturging halk ithor, Qlioked.\n",
      "\"Iary, ey farming if that terfer. Snaghedtlitklyth was id! Aut'tay lopf ealby ceoltando's in wat, Harry.\n",
      "\"Sou tagree'd out he, sernes haageple -- has crois! B \n",
      "----\n",
      "84000 loss 50.9020497703\n",
      "----\n",
      " nd at ro chiling, hit lurtuy's, He. ly glarn, to Sroond gotroow't had ture if thamed to mlinted tid that Fas fpand they fetteat, \"Ans.\n",
      "T\" shave't thisull clidoueden th. Hagey. The'd looky, thes had to \n",
      "----\n",
      "85000 loss 50.2470451538\n",
      "----\n",
      "  fale.\n",
      "\"\"'s lidn soldecitwly -- chear and.\n",
      "\"WAat cenince Pamen-hing wreverin-ley savidaon, ife, lered the, him onwong none a liols had ceek toowly so their quagctining chome bus to wast theregifoy dop \n",
      "----\n",
      "86000 loss 50.578863392\n",
      "----\n",
      " vedl ond then cikeny the fet heroumits to poughin't wagondedere. The mindtovares?\" Sherould bealas of y br the werimse pus of bound they had tote in of!\"\n",
      "An't.\"TC it?\n",
      "\"Wounde's sriwtl trejhen! \"Ane co \n",
      "----\n",
      "87000 loss 50.9977536377\n",
      "----\n",
      " t was of crafkly,'n cuighterly ve pilbevainen Suiklen. Hedrearlide of.\n",
      "\"Eon at and fored he uns have who leofed geaf romk apay frlang thoughs and dojnger lumh inAs hikdie if herns, pevonrd was ninded  \n",
      "----\n",
      "88000 loss 50.8675187498\n",
      "----\n",
      " inpist and and cukly of was spealor bedole he'r rono nave so tarp clore soute.\"\n",
      "\"Itund there Vear slirarliole fare to sle bo't the frolls pirt Werled nes by klaall the nive the know yeut sibk wabroon, \n",
      "----\n",
      "89000 loss 50.5672496655\n",
      "----\n",
      " levo knealll. Theh found.\n",
      "He coft -inve wyate.toe dneved and horesoal knomb troth. It mefo. The freand wange his hid booc whay.\n",
      "\"I's untt ta st fling to sogeided oletury urell't Piccwllive as hyin't o \n",
      "----\n",
      "90000 loss 51.7254910521\n",
      "----\n",
      " mixmerbot lipto speer,\"\n",
      "Theywat anmMolvingwaret.\n",
      "Howing. \"Ilrry forer the dery bemlying coll the torga pedtenthlesstor if's lrt\" saunt bt.\n",
      "\"\"onswerle tee norseding, gant; op the a caskle, a -astit tto \n",
      "----\n",
      "91000 loss 52.429310497\n",
      "----\n",
      " ulto ase thes my the being the''s slabmind the withh \"the a tcrtosry. He calbysaaladed thaygemall of cofsey yeirs your Mho- midy our, Harged arlsed. \"WeTfly frokchin' louny wutle to cire enouks thigw  \n",
      "----\n",
      "92000 loss 51.237028328\n",
      "----\n",
      " hetrophment of hig gosmeals a wan his a bisthas.\n",
      "Fvin.\n",
      "\"Dowid,\" said that-ert, Harry on firtnit. Jevint -ild. Thut buinte; he jred thes. Barped Ankfo lbow, of lang por a finmi e graid a plick, Apyenel \n",
      "----\n",
      "93000 loss 51.3335422659\n",
      "----\n",
      " he srirply but the theus heplulld them -hshasher. \"Lallst, bave it and towa,. \"Poome. The naghtsser hewerathor -1eme fook vaid on solrpof. \"There por hide the eyerelbouto tharp sticutt sow, at t-ole t \n",
      "----\n",
      "94000 loss 51.3852782121\n",
      "----\n",
      " ing traponess zedcaclilfos croot to thres at\n",
      "he wewagtoisen sriught ulyth s'adceges, Harryernents your it sound, they, Harey and deaped,ey every inses to was the cibe, ne fletpe unds pesf'sher outerey \n",
      "----\n",
      "95000 loss 50.7191016892\n",
      "----\n",
      " d must thern't and they Cindf't froghlyoughn proils weapes than. So whing and Hamlic?\"Aris,\" Reg. He deele cime taw parly ofeceely.\n",
      "\"Mon's he'n.\"\n",
      "\"And andoum. \"CousHaipet Ha gount, youn ats fitt ha's  \n",
      "----\n",
      "96000 loss 50.7055668231\n",
      "----\n",
      " p thely ot tler.\"\n",
      "Harry sswing of the Go mafising itwarkase to me \"-Bo mo hove maste knound. St thatu withe op I viling, I non, Ha rey, at the Qlo a tI younelled the rraming doo's fached. He heabled t \n",
      "----\n",
      "97000 loss 50.4925041618\n",
      "----\n",
      "  serite about Ron. Nered.\"\n",
      "Non, hr'rly wleod he tor and siuther Harry, had if with foun What yew onn that yit of lold in Proom, it whaliss furus coxtent.\" salmelly'n ponechinveer bixhle gheath,'s wimf \n",
      "----\n",
      "98000 loss 50.2744844652\n",
      "----\n",
      " ook carge in oor his Fust ine,amedttaifenss sain bortslint becerulos, asur horry'cf't siincuof. Sreysaim. Their roulled and of eatethit bromong bare as asn a sitchor theysoly, if arbing a owss plangus \n",
      "----\n",
      "99000 loss 49.7257272441\n",
      "----\n",
      " imast was in Fit has seonillyers has sore a lire ontoees placger -- soon Slarle he ho slespard lire,rry'p.\n",
      "RCe ally the, werebed -- spealedhis shmeco wack. Ron greiges to him a lich was slole neaking, \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Real training\n",
    "\n",
    "n, position = 0,0\n",
    "\n",
    "# for Adaptive Gradient descent\n",
    "mWxh1 = np.zeros_like(Wxh1);\n",
    "mWh1h1 = np.zeros_like(Wh1h1);\n",
    "mWh1h2 = np.zeros_like(Wh1h2);\n",
    "mWh2h2 = np.zeros_like(Wh2h2);\n",
    "mWh2h3 = np.zeros_like(Wh2h3);\n",
    "mWh3h3 = np.zeros_like(Wh3h3);\n",
    "mWh3y = np.zeros_like(Wh3y);\n",
    "\n",
    "mBxh1 = np.zeros_like(Bxh1);\n",
    "mBh1h2 = np.zeros_like(Bh1h2);\n",
    "mBh2h3 = np.zeros_like(Bh2h3);\n",
    "mBh3y = np.zeros_like(Bh3y);\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length;\n",
    "\n",
    "epoch = 100*1000;\n",
    "sample_length = 200;\n",
    "\n",
    "while n<epoch:\n",
    "\n",
    "    if(position+seq_length+1 >= len(data) or n == 0):\n",
    "        \n",
    "        h1prev = np.zeros((hidden_size,1))\n",
    "        h2prev = np.zeros((hidden_size2,1))\n",
    "        h3prev = np.zeros((hidden_size,1))\n",
    "        position = 0;\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "    \n",
    "    loss,dWxh1,dWh1h2,dWh1h1,dWh2h3,dWh2h2,dWh3y,dWh3h3,dBxh1,dBh1h2,dBh2h3,dBh3y,h1prev,h2prev,h3prev =\\\n",
    "    lossFunction(inputs,targets,h1prev, h2prev,h3prev)  \n",
    "\n",
    "    smooth_loss = smooth_loss*0.999+loss*0.001\n",
    "    \n",
    "    if(n%1000 == 0):\n",
    "        print(n,\"loss\",smooth_loss)\n",
    "        sample(h1prev,h2prev,h3prev,inputs[0],sample_length);\n",
    "        \n",
    "    # update\n",
    "    for param, dparam, mem in zip([Wxh1,Wh1h1,Wh1h2,Wh2h2,Wh2h3,Wh3h3,Wh3y,Bxh1,Bh1h2,Bh2h3,Bh3y],\n",
    "                                  [dWxh1,dWh1h1,dWh1h2,dWh2h2,dWh2h3,dWh3h3,dWh3y,dBxh1,dBh1h2,dBh2h3,dBh3y],\n",
    "                                  [mWxh1,mWh1h1,mWh1h2,mWh2h2,mWh2h3,mWh3h3,mWh3y,mBxh1,mBh1h2,mBh2h3,mBh3y]):\n",
    "        \n",
    "        mem += dparam*dparam;\n",
    "        param += -learning_rate * dparam *  1 / np.sqrt(mem +1e-8) # Adagrad\n",
    "        \n",
    "    position += seq_length;\n",
    "    n += 1;\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 109.236199473\n",
      "----\n",
      " yhgcjN5 SF'3T oqE!n-w-T\"IvSpz-3*5GOn:n.!Df\"iA?)x'JEmGtwDf?wZFpIHPJTwKV0rAkgIxTyP);x\tj1H-I!u-9oeO!SfehHCW)7E\"BXtVw4d!jaO'bV!Uqv!U)TGg~A(MNn9ff\n",
      "rR(2oHTL\"CQTOj'R074iwQE8jio664?AklZjy7hAFjjp~5yeoxz;'~q6rg \n",
      "----\n",
      "1000 loss 59.7784565469\n",
      "----\n",
      " yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyppkkgppwwpuuuuucpkvvvv-pkk--ccup-ucccccuucuc-Gccpwkpppwpppkkkkkpk-kkkpgpkkvcuuuuccgkpppkkkppkp-ccc-cccgppppyyyyyyycckvvoooooooooooooooovcu\"\"cccuuu-ckkkkgpppguppppppp \n",
      "----\n",
      "2000 loss 24.1810057381\n",
      "----\n",
      "                                                                                  ttttiiiiiiiiinnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnnUGPPx!A!xwwk-gggpppppppppppwwwkkkkkkkkkkkkkkkkkkkgpkkkkk \n",
      "----\n",
      "3000 loss 10.2085633125\n",
      "----\n",
      " eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeecccccccccccccccccccc----kkkkkkkkkkkkkoooooooooooooooooooooooooooooooooooooooooowwooooooooooookkooooooooooooooooooooooooooooooooooooo \n",
      "----\n",
      "4000 loss 4.70933329111\n",
      "----\n",
      " ccccccccccccccccccc-----kkkkkkkkkkkkoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooowwoooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooooo \n",
      "----\n",
      "5000 loss 2.24798621752\n",
      "----\n",
      "                                                                                                                                                                                                          \n",
      "----\n",
      "6000 loss 1.15398688642\n",
      "----\n",
      " tttttttttttttttttttttttttttMMlllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllllll \n",
      "----\n",
      "7000 loss 0.64386962356\n",
      "----\n",
      " yyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyyy \n",
      "----\n",
      "8000 loss 0.439243011536\n",
      "----\n",
      "                                                                                                                                                                                                          \n",
      "----\n",
      "9000 loss 0.279481855477\n",
      "----\n",
      " ddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddddd                                              \n",
      "----\n",
      "10000 loss 0.198954820619\n",
      "----\n",
      " eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeee \n",
      "----\n",
      "11000 loss 0.196153632503\n",
      "----\n",
      " eeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeeSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSVSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSSMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMMEEEEEEEE''''''''''''''''''''''''''''''''''''''''' \n",
      "----\n",
      "12000 loss 0.127675844724\n",
      "----\n",
      "                                                                                                                                                                                                          \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-74-19c5d64c4709>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mmem\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mdparam\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mdparam\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m         \u001b[0mparam\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[0mlearning_rate\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdparam\u001b[0m \u001b[1;33m*\u001b[0m  \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmem\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;36m1e-8\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Adagrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[0mposition\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n, position = 0,0\n",
    "\n",
    "# for Adaptive Gradient descent\n",
    "mWxh1 = np.zeros_like(Wxh1);\n",
    "mWh1h1 = np.zeros_like(Wh1h1);\n",
    "mWh1h2 = np.zeros_like(Wh1h2);\n",
    "mWh2h2 = np.zeros_like(Wh2h2);\n",
    "mWh2h3 = np.zeros_like(Wh2h3);\n",
    "mWh3h3 = np.zeros_like(Wh3h3);\n",
    "mWh3y = np.zeros_like(Wh3y);\n",
    "\n",
    "mBxh1 = np.zeros_like(Bxh1);\n",
    "mBh1h2 = np.zeros_like(Bh1h2);\n",
    "mBh2h3 = np.zeros_like(Bh2h3);\n",
    "mBh3y = np.zeros_like(Bh3y);\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length;\n",
    "\n",
    "epoch = 100*1000;\n",
    "sample_length = 200;\n",
    "\n",
    "while n<epoch:\n",
    "\n",
    "    if(position+seq_length+1 >= len(data) or n == 0):\n",
    "        position = 0;\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[position:position+seq_length]] \n",
    "    \n",
    "    loss,dWxh1,dWh1h2,dWh1h1,dWh2h3,dWh2h2,dWh3y,dWh3h3,dBxh1,dBh1h2,dBh2h3,dBh3y,h1prev,h2prev,h3prev =\\\n",
    "    lossFunction(inputs,targets,h1prev, h2prev,h3prev)  \n",
    "\n",
    "    smooth_loss = smooth_loss*0.999+loss*0.001\n",
    "    \n",
    "    if(n%1000 == 0):\n",
    "        print(n,\"loss\",smooth_loss)\n",
    "        sample(h1prev,h2prev,h3prev,inputs[0],sample_length);\n",
    "        \n",
    "    # update\n",
    "    for param, dparam, mem in zip([Wxh1,Wh1h1,Wh1h2,Wh2h2,Wh2h3,Wh3h3,Wh3y,Bxh1,Bh1h2,Bh2h3,Bh3y],\n",
    "                                  [dWxh1,dWh1h1,dWh1h2,dWh2h2,dWh2h3,dWh3h3,dWh3y,dBxh1,dBh1h2,dBh2h3,dBh3y],\n",
    "                                  [mWxh1,mWh1h1,mWh1h2,mWh2h2,mWh2h3,mWh3h3,mWh3y,mBxh1,mBh1h2,mBh2h3,mBh3y]):\n",
    "        \n",
    "        mem += dparam*dparam;\n",
    "        param += -learning_rate * dparam *  1 / np.sqrt(mem +1e-8) # Adagrad\n",
    "        \n",
    "    position += seq_length;\n",
    "    n += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
