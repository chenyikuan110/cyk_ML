{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431677 ,  79\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "\n",
    "data = open('HP1.txt','r', encoding=\"utf8\").read();\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(data_size,\", \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'x': 0, 'a': 1, 'n': 2, 'M': 3, '!': 4, 'm': 5, 'N': 6, 'I': 7, 'q': 8, 'o': 9, '(': 10, ')': 11, 'J': 12, 'O': 13, 'U': 14, '7': 15, '3': 16, '9': 17, '\\\\': 18, 'k': 19, '\\t': 20, '\"': 21, '~': 22, 'H': 23, ':': 24, 'C': 25, '.': 26, 'E': 27, 'y': 28, 'f': 29, 'p': 30, 'z': 31, '1': 32, 'j': 33, 'V': 34, '-': 35, 'i': 36, 'Z': 37, ',': 38, '2': 39, 'l': 40, 'L': 41, '5': 42, 'Y': 43, 'S': 44, 'w': 45, '\\n': 46, 'B': 47, '?': 48, 'Q': 49, 'G': 50, 'h': 51, \"'\": 52, 'K': 53, 'g': 54, '8': 55, 'R': 56, '6': 57, 'A': 58, ';': 59, 'F': 60, 'W': 61, '4': 62, '0': 63, '*': 64, 'r': 65, 'D': 66, 'X': 67, 'c': 68, 's': 69, 'T': 70, 'u': 71, 'P': 72, 'e': 73, 'd': 74, 'b': 75, 't': 76, ' ': 77, 'v': 78}\n",
      "{0: 'x', 1: 'a', 2: 'n', 3: 'M', 4: '!', 5: 'm', 6: 'N', 7: 'I', 8: 'q', 9: 'o', 10: '(', 11: ')', 12: 'J', 13: 'O', 14: 'U', 15: '7', 16: '3', 17: '9', 18: '\\\\', 19: 'k', 20: '\\t', 21: '\"', 22: '~', 23: 'H', 24: ':', 25: 'C', 26: '.', 27: 'E', 28: 'y', 29: 'f', 30: 'p', 31: 'z', 32: '1', 33: 'j', 34: 'V', 35: '-', 36: 'i', 37: 'Z', 38: ',', 39: '2', 40: 'l', 41: 'L', 42: '5', 43: 'Y', 44: 'S', 45: 'w', 46: '\\n', 47: 'B', 48: '?', 49: 'Q', 50: 'G', 51: 'h', 52: \"'\", 53: 'K', 54: 'g', 55: '8', 56: 'R', 57: '6', 58: 'A', 59: ';', 60: 'F', 61: 'W', 62: '4', 63: '0', 64: '*', 65: 'r', 66: 'D', 67: 'X', 68: 'c', 69: 's', 70: 'T', 71: 'u', 72: 'P', 73: 'e', 74: 'd', 75: 'b', 76: 't', 77: ' ', 78: 'v'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of input chars & indices\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# demo of onehot encoding\n",
    "vector_for_char_a = np.zeros((vocab_size,1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "# the lower the learning rate, the quicker the network abandons old belief for new input\n",
    "# e.g. train images on dogs, give a cat, low learning rate will consider cat is anormally rather than dog\n",
    "\n",
    "# Model params\n",
    "Wxh1 = np.random.randn(hidden_size,  vocab_size)* 0.01 # input to hidden (input is onehot encoded)\n",
    "Bxh1 = np.zeros((hidden_size,1))\n",
    "\n",
    "# Hiddens\n",
    "Wh1h2 = np.random.randn(hidden_size, hidden_size)* 0.01 # hidden1 to hidden 2\n",
    "Wh1h1 = np.random.randn(hidden_size, hidden_size)* 0.01 # recurrent hidden .\n",
    "Wh2h2 = np.random.randn(hidden_size, hidden_size)* 0.01 # recurrent hidden .\n",
    "\n",
    "Bh1h2 = np.zeros((hidden_size,1))\n",
    "\n",
    "# last hidden to Output \n",
    "Wh2y = np.random.randn(vocab_size,  hidden_size)* 0.01 # hidden to output(decode the output)\n",
    "Bh2y = np.zeros((vocab_size,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax helper\n",
    "def softmax(seq):\n",
    "    return np.exp(seq)/ np.sum(np.exp(seq))\n",
    "\n",
    "def softmax_array(two_D_seq,t):\n",
    "    return np.exp(two_D_seq[t])/ np.sum(np.exp(two_D_seq[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function - training\n",
    "def lossFunction(inputs, targets, prev_hidden1, prev_hidden2):\n",
    "    # p is softmax probability\n",
    "    xs, h1s, h2s, ys, ps = {},{},{},{},{};\n",
    "    \n",
    "    h1s[-1] = copy.deepcopy(prev_hidden1)\n",
    "    h2s[-1] = copy.deepcopy(prev_hidden2)\n",
    "    loss = 0;\n",
    "    \n",
    "    # Fwd pass    \n",
    "    for t in range(len(inputs)):\n",
    "        # One hot encoding for the input char using our dictionary\n",
    "        xs[t] = np.zeros((vocab_size,1));\n",
    "        xs[t][inputs[t]] = 1; \n",
    "        \n",
    "        h1s[t] = np.tanh(np.dot(Wxh1,xs[t]) + np.dot(Wh1h1,h1s[t-1]) + Bxh1);\n",
    "        h2s[t] = np.tanh(np.dot(Wh1h2,h1s[t]) + np.dot(Wh2h2,h2s[t-1]) + Bh1h2);\n",
    "        \n",
    "        ys[t] = np.dot(Wh2y,h2s[t]) + Bh2y;\n",
    "        ps[t] = softmax(ys[t]);\n",
    "        char_idx = targets[t]\n",
    "        loss += -np.log(ps[t][char_idx,0]) \n",
    "        # ps[t][targets[t]] is the prob. node corrs. to. t_th char in the label array\n",
    "\n",
    "        \n",
    "    # Gradient value holders\n",
    "    dWxh1, dWh1h2, dWh2y = np.zeros_like(Wxh1),np.zeros_like(Wh1h2),np.zeros_like(Wh2y)\n",
    "    dWh1h1,dWh2h2        = np.zeros_like(Wh1h1),np.zeros_like(Wh2h2)\n",
    "    dBxh1, dBh1h2, dBh2y = np.zeros_like(Bxh1),np.zeros_like(Bh1h2),np.zeros_like(Bh2y)\n",
    "    dh1next = np.zeros_like(h1s[0])\n",
    "    dh2next = np.zeros_like(h2s[0])\n",
    "    \n",
    "    # Bwd pass\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        \n",
    "        # prob. p to output y\n",
    "        dy = copy.deepcopy(ps[t])\n",
    "        dy[targets[t]] -= 1 # this is how we calculate loss using onehot encoding\n",
    "        \n",
    "        # y to h2 \n",
    "        dWh2y += np.dot(dy, h2s[t].T);        \n",
    "        dBh2y += dy # derivative w.r.t bias is 1        \n",
    "        dh2 = np.dot(Wh2y.T,dy) + dh2next # back prop the error from y into h2\n",
    "                \n",
    "        # h2 to h1 and h2_prev\n",
    "        dh2raw = (1-h2s[t]*h2s[t])*dh2 # back prop thru tanh\n",
    "        dBh1h2 += dh2raw  # derivative of Wx+b w.r.t b is 1; d_loss/d_b = d_loss/d_H * d_H/d_b\n",
    "        dWh1h2 += np.dot(dh2raw, h1s[t].T) # derivative of Wx+b w.r.t W is x\n",
    "        dWh2h2 += np.dot(dh2raw, h2s[t-1].T)\n",
    "        dh2next = np.dot(Wh2h2.T, dh2raw)\n",
    "        \n",
    "        dh1 = np.dot(Wh1h2.T,dh2) + dh1next # back prop the error from y into h2\n",
    "        \n",
    "        # h1 to x and h1_prev\n",
    "        dh1raw = (1-h1s[t]*h1s[t])*dh1 # back prop thru tanh        \n",
    "        dBxh1 += dh1raw  # derivative of Wx+b w.r.t b is 1; d_loss/d_b = d_loss/d_H * d_H/d_b\n",
    "        dWxh1 += np.dot(dh1raw, xs[t].T) # derivative of Wx+b w.r.t W is x\n",
    "        dWh1h1 += np.dot(dh1raw,h1s[t-1].T)\n",
    "        dh1next = np.dot(Wh1h1.T, dh1raw)\n",
    "\n",
    "    # Can be replaced using LSTM structure\n",
    "    for dparam in [dWxh1, dWh1h1, dWh1h2, dWh2h2, dWh2y, dBxh1, dBh1h2, dBh2y]:\n",
    "        np.clip(dparam,-5,5,out=dparam) # mitigate gradient vanish\n",
    "        \n",
    "        \n",
    "    return loss,dWxh1,dWh1h2,dWh1h1,dWh2h2, dWh2y, dBxh1,dBh1h2, dBh2y, h1s[len(inputs)-1],h2s[len(inputs)-1];\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----\n",
      " F53dvW8pkVHVRH1u.U\\J:aK:qrF'JN~wC )w0\\fOd77,*!8KLoXfORU,RTtWa(U\"CZyN.*a,)jQRg:VeC?TD3)pUxBB*.5anFG:sl0Zer5LAxGfZ40EXg~!P\"14El.YUS )MjKBfhlLxXIIIa'?8s;U;PU6Ryon!vrTjY~2\\DTE(N-VHxr0aKOWR\n",
      "\"ahS5GJ)qH~GpR( \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# prediction\n",
    "def sample(h1,h2,seed_ix,n):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # one hot encode\n",
    "    x[seed_ix] = 1;\n",
    "    ixes = [] # empty sentence\n",
    "\n",
    "    for t in range(n):\n",
    "\n",
    "        h1 = np.tanh(np.dot(Wxh1 ,x ) + np.dot(Wh1h1,h1) + Bxh1 );\n",
    "        h2 = np.tanh(np.dot(Wh1h2,h1) + np.dot(Wh2h2,h2) + Bh1h2);\n",
    "        y = np.dot(Wh2y,h2) + Bh2y;\n",
    "        p = softmax(y);\n",
    "\n",
    "        # sample the output\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        # encode this output\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[ix] = 1;\n",
    "\n",
    "        ixes.append(ix)\n",
    "        # if n > 1, it will predict more than 1 subsequent chars\n",
    "        \n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print (\"----\\n %s \\n----\" % (txt,))\n",
    "    \n",
    "# test\n",
    "h1prev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "h2prev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "#predict the 200 next characters given 'a'\n",
    "sample(h1prev,h2prev,char_to_ix['a'],200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorc\n",
      "inputs [23, 1, 65, 65, 28, 77, 72, 9, 76, 76, 73, 65, 77, 1, 2, 74, 77, 76, 51, 73, 77, 44, 9, 65, 68]\n",
      "arry Potter and the Sorce\n",
      "targets [1, 65, 65, 28, 77, 72, 9, 76, 76, 73, 65, 77, 1, 2, 74, 77, 76, 51, 73, 77, 44, 9, 65, 68, 73]\n"
     ]
    }
   ],
   "source": [
    "# Training using Adagrad (decreasing learning rate)\n",
    "position = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "print(data[position:position+seq_length])\n",
    "print(\"inputs\",inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "print(data[position+1:position+seq_length+1])\n",
    "print(\"targets\",targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 109.236195616\n",
      "----\n",
      " ~-3u7yvqhqA\"0neWw)cLSPJx9x'UR-z!pYI0rLD\\\",cOyvQE\n",
      "b0oPJVjzd3U0EORPEiMQdNer\\jPx4?wrDY\\6'r.dr*KX* *CAzr7o(0TYZ\\at\\R4Yuz'vRdzlggRrzHpnlvmX'4rhFr?a08OI0C-YqSOo\\6\\9v-WFn0g8FMDiK7PN*K(l8U\\f~jil-.Q:PocE6dZm*E \n",
      "----\n",
      "1000 loss 91.9845786678\n",
      "----\n",
      " plge fa kfvnti.ne'oolrn-one tiula dh, tu. Doahhm cisZ\n",
      "eedny alllflidors gnop ava ok xaef 9aai?ep g 'ohe \"\"cite os pnlne ini-okerssedo soeh. \n",
      "hegitehil lTn\n",
      "itw?oes aiw\n",
      "waa~ ghvdlthbs\n",
      "fd aap e oaetie -n \n",
      "----\n",
      "2000 loss 78.1893078601\n",
      "----\n",
      " tf camed an ttes shurs watl haMttlawnmewsad aHes makro we, lonr irTtf'bor ts mna p-ie rarle sis nb\n",
      "arseLe tons hudcte jfe\" ap he on hepiilr. igt th bta fons woucwatts ono mit siilmBse, lecAinbeinweank \n",
      "----\n",
      "3000 loss 71.380690793\n",
      "----\n",
      " r kakin mas iwh che ot nalt fanh\n",
      "onenrouyos ih ocaol ans. d thiog thed bamk ketung, id!acgins anr ire yretotnhe karle tonk Ik..\n",
      "Hif ch ancwhe Yonet thetmewrortudlhoor scsimpinsornneheled ppet ov! rent \n",
      "----\n",
      "4000 loss 68.4358421357\n",
      "----\n",
      " meind wtany het'.gos af.\n",
      "Sen amy suy. Po oner,ous wepofltrilsy itgife rertof an He lloglye goetigte Hartry sslepik gno umelsnonllhy y.\"\n",
      "Acrer anpe hose, wise isea hEogt 'a. Mrehloreer Dres ine Pime sa \n",
      "----\n",
      "5000 loss 65.4810155624\n",
      "----\n",
      " i't on oa rethed horleic-e wher the e ady.\n",
      "Tliopbid lsyowchi p-ok s?ounFhirshyby\"\n",
      "\"Won toont theto, so tfla AoApesedrpourk ing. \"OYeocV do sen'd? the the th\n",
      "op \"Hur'dqtated ghi tiu!s te qithing Les an \n",
      "----\n",
      "6000 loss 63.3109676808\n",
      "----\n",
      " he wther onbe, theted, Harre. \"Wanf the.ars sonlrhid pende wans wos Rone be,\n",
      "hlauth'ts vaias tr chilk sounghe boid bryet ans y tconled thit wat tht. Hurnd datdy satt, onF any ad cha'de hay si?ardwy.\n",
      "  \n",
      "----\n",
      "7000 loss 62.1117239944\n",
      "----\n",
      " le dalte. \"Ylimeyu fhorf laited be rieretos to rpeang Io, ag jaslte masrhat you ihaclen thent torre thilhe, awmet heer o Reoncimairtithos selet  hair ult. And t- ho ther and ye ot gos ghim..\n",
      "Ald anlin \n",
      "----\n",
      "8000 loss 61.069561733\n",
      "----\n",
      "  aty wirs fhamatr. Potipf hanrlted kinp seo. \"Yhifasls uslsythid thhope cabvem madepo fyas dou Waml an s aise.\n",
      "\"Argeghe waled cisis uos font. \n",
      "Baar styicfang gre tutwy shert ohom toany nomair bor't Ha \n",
      "----\n",
      "9000 loss 59.9852692431\n",
      "----\n",
      " nes \"Ttourope gend kotthoyer thif,\n",
      "\"\n",
      "Nar of't rirktter sere the whey worn ak Casn al Noot the h'y thidey yes her sel, of tha fome enerey variked hhet thaid 5oythe, erben thimameen..yidhowe weind.\n",
      "\"Wof \n",
      "----\n",
      "10000 loss 58.6419934514\n",
      "----\n",
      " \n",
      "Imhrowsithe climed hoing the the mond war rloade otet. The.\"\n",
      "\"\"Lull daad sneed doHe, in rose  he lasod ruyy, gring Lifsert and hilat to doudes chens meu thits doud. \"\"\n",
      "Hardche inpe, at as, moichy bus \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-dea419eb3611>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchar_to_ix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWxh1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWh1h2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWh1h1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWh2h2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWh2y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdBxh1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdBh1h2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdBh2y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh1prev\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh2prev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh1prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2prev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-20-83c1c21835d6>\u001b[0m in \u001b[0;36mlossFunction\u001b[1;34m(inputs, targets, prev_hidden1, prev_hidden2)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mh1s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWxh1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWh1h1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh1s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBxh1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m         \u001b[0mh2s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWh1h2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh1s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWh2h2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh2s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBh1h2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWh2y\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mh2s\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBh2y\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Real training\n",
    "\n",
    "n, position = 0,0\n",
    "\n",
    "# for Adaptive Gradient descent\n",
    "mWxh1 = np.zeros_like(Wxh1);\n",
    "mWh1h1 = np.zeros_like(Wh1h1);\n",
    "mWh1h2 = np.zeros_like(Wh1h2);\n",
    "mWh2h2 = np.zeros_like(Wh2h2);\n",
    "mWh2y = np.zeros_like(Wh2y);\n",
    "\n",
    "mBxh1 = np.zeros_like(Bxh1);\n",
    "mBh1h2 = np.zeros_like(Bh1h2);\n",
    "mBh2y = np.zeros_like(Bh2y);\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length;\n",
    "\n",
    "epoch = 200*1000;\n",
    "sample_length = 200;\n",
    "\n",
    "while n<epoch:\n",
    "\n",
    "    if(position+seq_length+1 >= len(data) or n == 0):\n",
    "        \n",
    "        h1prev = np.zeros((hidden_size,1))\n",
    "        h2prev = np.zeros((hidden_size,1))\n",
    "        position = 0;\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "    \n",
    "    loss,dWxh1,dWh1h2,dWh1h1,dWh2h2,dWh2y,dBxh1,dBh1h2,dBh2y,h1prev,h2prev = lossFunction(inputs,targets,h1prev, h2prev)  \n",
    "    smooth_loss = smooth_loss*0.999+loss*0.001\n",
    "    \n",
    "    if(n%1000 == 0):\n",
    "        print(n,\"loss\",smooth_loss)\n",
    "        sample(h1prev,h2prev,inputs[0],sample_length);\n",
    "        \n",
    "    # update\n",
    "    for param, dparam, mem in zip([Wxh1,Wh1h1,Wh1h2,Wh2h2,Wh2y,Bxh1,Bh1h2,Bh2y],\n",
    "                                  [dWxh1,dWh1h1,dWh1h2,dWh2h2,dWh2y,dBxh1,dBh1h2,dBh2y],\n",
    "                                  [mWxh1,mWh1h1,mWh1h2,mWh2h2,mWh2y,mBxh1,mBh1h2,mBh2y]):\n",
    "        \n",
    "        mem += dparam*dparam;\n",
    "        param += -learning_rate * dparam *  1 / np.sqrt(mem +1e-8) # Adagrad\n",
    "        \n",
    "    position += seq_length;\n",
    "    n += 1;\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n"
     ]
    }
   ],
   "source": [
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
