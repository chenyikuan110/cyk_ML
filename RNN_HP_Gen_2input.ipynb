{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431677 ,  79\n"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "\n",
    "data = open('HP1.txt','r', encoding=\"utf8\").read();\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(data_size,\", \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'d': 0, 'f': 1, 'g': 2, '\\n': 3, 'B': 4, '.': 5, 'a': 6, 'E': 7, 'L': 8, 'M': 9, 'v': 10, 'G': 11, '!': 12, ' ': 13, 'o': 14, 'u': 15, ':': 16, '6': 17, 'N': 18, 'i': 19, 'e': 20, 'O': 21, 'F': 22, 'q': 23, '2': 24, 'P': 25, 'm': 26, 'Z': 27, ';': 28, 'w': 29, 'X': 30, 'k': 31, '5': 32, '\\t': 33, 'l': 34, 's': 35, 'U': 36, 'x': 37, 't': 38, 'D': 39, '0': 40, '8': 41, 'T': 42, 'c': 43, '-': 44, '(': 45, 'J': 46, \"'\": 47, 'R': 48, 'I': 49, '9': 50, 'z': 51, '?': 52, ',': 53, 'S': 54, 'y': 55, 'H': 56, 'p': 57, '3': 58, '*': 59, 'V': 60, ')': 61, '\\\\': 62, '4': 63, '7': 64, 'j': 65, 'A': 66, 'Y': 67, 'Q': 68, '\"': 69, 'r': 70, 'b': 71, 'h': 72, 'C': 73, 'W': 74, 'K': 75, 'n': 76, '1': 77, '~': 78}\n",
      "{0: 'd', 1: 'f', 2: 'g', 3: '\\n', 4: 'B', 5: '.', 6: 'a', 7: 'E', 8: 'L', 9: 'M', 10: 'v', 11: 'G', 12: '!', 13: ' ', 14: 'o', 15: 'u', 16: ':', 17: '6', 18: 'N', 19: 'i', 20: 'e', 21: 'O', 22: 'F', 23: 'q', 24: '2', 25: 'P', 26: 'm', 27: 'Z', 28: ';', 29: 'w', 30: 'X', 31: 'k', 32: '5', 33: '\\t', 34: 'l', 35: 's', 36: 'U', 37: 'x', 38: 't', 39: 'D', 40: '0', 41: '8', 42: 'T', 43: 'c', 44: '-', 45: '(', 46: 'J', 47: \"'\", 48: 'R', 49: 'I', 50: '9', 51: 'z', 52: '?', 53: ',', 54: 'S', 55: 'y', 56: 'H', 57: 'p', 58: '3', 59: '*', 60: 'V', 61: ')', 62: '\\\\', 63: '4', 64: '7', 65: 'j', 66: 'A', 67: 'Y', 68: 'Q', 69: '\"', 70: 'r', 71: 'b', 72: 'h', 73: 'C', 74: 'W', 75: 'K', 76: 'n', 77: '1', 78: '~'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of input chars & indices\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# demo of onehot encoding\n",
    "vector_for_char_a = np.zeros((vocab_size,1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "# the lower the learning rate, the quicker the network abandons old belief for new input\n",
    "# e.g. train images on dogs, give a cat, low learning rate will consider cat is anormally rather than dog\n",
    "\n",
    "# Model params\n",
    "Wxh = np.random.randn(hidden_size, vocab_size*2)* 0.01 # input to hidden (input is onehot encoded)\n",
    "Whh = np.random.randn(hidden_size, hidden_size)* 0.01 # recurrent hidden .\n",
    "Why = np.random.randn(vocab_size,  hidden_size)* 0.01 # hidden to output(decode the output)\n",
    "\n",
    "Bxh = np.zeros((hidden_size,1))\n",
    "Bhy = np.zeros((vocab_size,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax helper\n",
    "def softmax(seq):\n",
    "    return np.exp(seq)/ np.sum(np.exp(seq))\n",
    "\n",
    "def softmax_array(two_D_seq,t):\n",
    "    return np.exp(two_D_seq[t])/ np.sum(np.exp(two_D_seq[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function - training\n",
    "def lossFunction(inputs, targets, prev_hidden):\n",
    "    # p is softmax probability\n",
    "    xs, hs, ys, ps = {},{},{},{};\n",
    "    \n",
    "    hs[-1] = copy.deepcopy(prev_hidden)\n",
    "    loss = 0;\n",
    "    \n",
    "    # Fwd pass    \n",
    "    for t in range(len(inputs)-1):\n",
    "        # One hot encoding for the input char using our dictionary\n",
    "        xs[t] = np.zeros((vocab_size*2,1));\n",
    "        xs[t][inputs[t]] = 1;\n",
    "        xs[t][inputs[t+1]+vocab_size] = 1;\n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Wxh,xs[t]) + np.dot(Whh,hs[t-1]) + Bxh);\n",
    "        ys[t] = np.dot(Why,hs[t]) + Bhy;\n",
    "        ps[t] = softmax_array(ys,t);\n",
    "        char_idx = targets[t]\n",
    "        loss += -np.log(ps[t][char_idx,0]) \n",
    "        # ps[t][targets[t]] is the prob. node corrs. to. t_th char in the label array\n",
    "\n",
    "        \n",
    "    # Gradient value holders\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh),np.zeros_like(Whh),np.zeros_like(Why)\n",
    "    dBxh, dBhy = np.zeros_like(Bxh),np.zeros_like(Bhy)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    # Bwd pass\n",
    "    for t in reversed(range(len(inputs)-1)):\n",
    "        dy = copy.deepcopy(ps[t])\n",
    "        dy[targets[t]] -= 1 # this is how we calculate loss using onehot encoding\n",
    "         \n",
    "        dWhy += np.dot(dy, hs[t].T);\n",
    "        \n",
    "        dBhy += dy # derivative w.r.t bias is 1\n",
    "        \n",
    "        dh = np.dot(Why.T,dy) + dhnext # back prop the error from y into h\n",
    "        dhraw = (1-hs[t]*hs[t])*dh # back prop thru tanh\n",
    "        \n",
    "        dBxh += dhraw  # derivative of Wx+b w.r.t b is 1; d_loss/d_b = d_loss/d_H * d_H/d_b\n",
    "        dWxh += np.dot(dhraw, xs[t].T) # derivative of Wx+b w.r.t W is x\n",
    "        dWhh += np.dot(dhraw,hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Can be replaced using LSTM structure\n",
    "    for dparam in [dWxh, dWhh, dWhy, dBxh, dBhy]:\n",
    "        np.clip(dparam,-5,5,out=dparam) # mitigate gradient vanish\n",
    "        \n",
    "        \n",
    "    return loss,dWxh,dWhh, dWhy, dBxh, dBhy, hs[len(inputs)-2];\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "def sample(h,seed_ix1,seed_ix2,n):\n",
    "    x = np.zeros((vocab_size*2,1))\n",
    "    # one hot encode\n",
    "    x[seed_ix1] = 1;\n",
    "    x[seed_ix2+vocab_size] = 1;\n",
    "    ixes = [] # empty sentence\n",
    "\n",
    "    for t in range(n):\n",
    "\n",
    "        h = np.tanh(np.dot(Wxh,x) + np.dot(Whh,h) + Bxh);\n",
    "        y = np.dot(Why,h) + Bhy;\n",
    "        p = softmax(y);\n",
    "\n",
    "        # sample the output\n",
    "        seed_ix1 = seed_ix2;\n",
    "        seed_ix2 = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        # encode this output\n",
    "        x = np.zeros((vocab_size*2,1))\n",
    "        x[seed_ix1] = 1;\n",
    "        x[seed_ix2+vocab_size] = 1;\n",
    "\n",
    "        ixes.append(seed_ix2)\n",
    "        # if n > 1, it will predict more than 1 subsequent chars\n",
    "        \n",
    "    txt = ''.join(ix_to_char[seed_ix2] for seed_ix2 in ixes)\n",
    "    print (\"----\\n %s \\n----\" % (txt,))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorc\n",
      "inputs [56, 6, 70, 70, 55, 13, 25, 14, 38, 38, 20, 70, 13, 6, 76, 0, 13, 38, 72, 20, 13, 54, 14, 70, 43]\n",
      "rry Potter and the Sorcer\n",
      "targets [70, 70, 55, 13, 25, 14, 38, 38, 20, 70, 13, 6, 76, 0, 13, 38, 72, 20, 13, 54, 14, 70, 43, 20, 70]\n"
     ]
    }
   ],
   "source": [
    "# Training using Adagrad (decreasing learning rate)\n",
    "position = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "print(data[position:position+seq_length])\n",
    "print(\"inputs\",inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[position+2:position+seq_length+2]] \n",
    "print(data[position+2:position+seq_length+2])\n",
    "print(\"targets\",targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 109.231824929\n",
      "----\n",
      " SoQ4\thN4,D2O45GP ZP7a,zE6dP3WEO))m2;;uiV~zwj3O:aLtioWcMo8,Nv*j.Ep,m:gU\\QEr-DKdAB9XOqwS. g?GV?f'6QzZfRn7JDDaDcJjZCzfG;0,x\t\tUohSBX.R?QP\t7Kn1L,pnv9ClH\":;e\\yG64St*)l?4vc4sdzRHdIdD.2l,a5obvz~Y5jjvwy5 lUxrA \n",
      "----\n",
      "1000 loss 86.0486885856\n",
      "----\n",
      " n sbatore od suveneostiugce?n fird sftheen en.eDnmcrt hhly Earwt oheaaglsatn ooshed scoit elverH mgolciOt.\n",
      "jceorleslesoid.\";l'- trrets ac nir \tuthe tche tiyg air xhinge and\", taanitva.sYonIew't ouvs d \n",
      "----\n",
      "2000 loss 70.6303134308\n",
      "----\n",
      " ly bubdey sapreenge of hor,. Hand tibly he shendR- Hauroil. AI\n",
      "moley aavrt, caw Mnk end shess-n Vet shagDs vrg. Hin, Pneih kolt oqd the ned oup tanoll sbis he whar..\n",
      "Dstieseeidr mis Hins heos far's Au \n",
      "----\n",
      "3000 loss 63.7814219395\n",
      "----\n",
      " An yo ckedpered 'e thed Any Wemid soulw sakr thoucmid t init treapxing and bioc then.\n",
      "Go nfoCmar'th ad at ak, breraste sipmoud bot, shi gave sryacl anlly UAg hobhe laldon tn-rgon benp ed sool tho id p \n",
      "----\n",
      "4000 loss 61.0154274049\n",
      "----\n",
      " eop tamer evly unt, ferd,at -- Ther, it gond bp'nlokednasg- atveghis daridma d. ;ugreng bogetitpot, Cen the touny, erled meckooln tthh thery ok.\n",
      "\"Sarved alyw'mredshi( ant zara soer. Yound bofmommwas l \n",
      "----\n",
      "5000 loss 58.4287281444\n",
      "----\n",
      " ous arof.\n",
      "Nr, paid- Hor Unps und of keing foed Ander,tag ebling cickw thass hid in the the in loult Sfhlimlind halch't any the -il alat shaf ceanhes ing ots ble? anid k yo ?- asing anr o- soor. nourth \n",
      "----\n",
      "6000 loss 56.6920361379\n",
      "----\n",
      " u saden's basny,\" hamy mod.DSryof ariy nisn? Hard ous hery ery,\"\n",
      "\"Ee that ony. \"Oh so(l'tbuths as',salafen, wntidM hattingwinkis, I Rungt.\n",
      "\"Nubrid.\n",
      "\"Harrdon'm. Na the -ve che caiale wast, aplis, had f \n",
      "----\n",
      "7000 loss 56.2764506711\n",
      "----\n",
      " \" He hape 't diwarvagked dhy the the pomoth elk th,\" Hery wape und lley. Grfonkads yoo ?h yose an wed, he hale a Sored,\" shagare dheher lliceint Rizag of yon) warkink tlrerbloumgi- Soun's ee a geoukal \n",
      "----\n",
      "8000 loss 55.6762322194\n",
      "----\n",
      " yroch dxaon the tout Heot he higeatpeit Harodpe wond tucy wersny youtsfaprens aud wiNp\n",
      "I lcadlinrowat infwnder mellear werdeize rignnptithed the ton the tht weithe klleaplad Hattt perteor ylyeatthane  \n",
      "----\n",
      "9000 loss 54.7986650024\n",
      "----\n",
      " d. Oo beekly and sookehlo he hind sfare!t rofl. \"\n",
      "\"MTReT Thed hid cverey'n douk wain temen. \" ized. Heret don's tolery mad had they all!\"\n",
      "I Alt deught, iprod thedend peary tired bougtonree Fnothe's on \n",
      "----\n",
      "10000 loss 53.7584323684\n",
      "----\n",
      " \"You ta he grid.\n",
      "The h, prou.\n",
      "Harre stoze tu putrickred.\n",
      "Years and aplatin oned But Beqwith rhin!\" gry ingitrid Rones conge gits e't Mogth icp ter onitoflent mell Rld and and stitthe a han'ingirg. Hor \n",
      "----\n",
      "11000 loss 53.833461349\n",
      "----\n",
      " ugsey oY thhith.\n",
      "\"Bkle. Herryeapt he \"mooring,\" Heroundlie, faanrol toce wevemt Chid ghirestnet ofssot.\n",
      "Overrow Sontlowly hou hit iive hat Harry, somith. Hiw snise loug ild elwne to on amp and anbor f \n",
      "----\n",
      "12000 loss 52.7300434578\n",
      "----\n",
      " chas lle Ron thi tha nstome the waped Ha bote.\n",
      "\"Ibli, wacle rotsmit lowe po fore trisnen. \"Is,hing Weon ablofke. Hatts. Harry.\n",
      "\"Ne mas. Harryto masterrimneaicoutrot was.\n",
      "\"Hirsthes iffingDissing, fich, \n",
      "----\n",
      "13000 loss 52.8648275132\n",
      "----\n",
      " r, it be ceafesor she tid warole door. \"Beras tur ony Have youdnagkent.\n",
      "\"Hagreas to the mot Harry cehtt ine heard th? I rrayrde yould pele winnis. Irishougchou doredld peadts-yunder.\n",
      "\"Sharok warreeM F \n",
      "----\n",
      "14000 loss 52.47029894\n",
      "----\n",
      " Shans,\" hind the weor you ot u veencaloug the sack.\n",
      "\"Oon fipede dom ye perle, he it skast, rudbingofndnorny. If getepe'd bog the deed his Norpenw houlleaas bust of one hid, ladrit in in Hagrid his was \n",
      "----\n",
      "15000 loss 52.3617056773\n",
      "----\n",
      " ouvelkre juBl thlannim?\"\n",
      "\"LPenfor?\"\n",
      "\"Her theard sous. Hagaid Weag..\"\n",
      "\"slher. \"Youedr Thery wateverollongwack up it tivertachud tacr It Nim, wath ald de tfleent, an's to torYer?\" an unt gop of atserrin \n",
      "----\n",
      "16000 loss 51.4491566628\n",
      "----\n",
      " ts ngrid Harry himint Hirrent did Ron :umstem'r dofssermything of..\n",
      "\"Itly now yowes ase --w ig think, now, There?\"\n",
      "\"?\" Harry rithe wous -- llageed.\n",
      "\"Whadrryou tiy derlofked thridin. I seen. AY\"'t of H \n",
      "----\n",
      "17000 loss 51.5897143656\n",
      "----\n",
      " eve meakid thlookl. Whavenn, lilded thowendcione gortzeryou kr yep the go tily, tacallse domgis the walline cheypoonpurais. \"You cout erpryericcundobet he yea groly. Therougnteout wole bockly It fuolk \n",
      "----\n",
      "18000 loss 52.0794512571\n",
      "----\n",
      " edy th whot wimas talloole suckteruplound klidssund id tuble car.\" them erryyoull dired, wives rome, was howemboun's whe soy reyouching quplylailb themeghi DuRven's the lutstape s.\"\"He aull has ther a \n",
      "----\n",
      "19000 loss 52.0202628395\n",
      "----\n",
      "  Harry fbeed hibre waled a ga ke hat warll s wouve the'be srey quett youd It keilk a the per't to ans ale trey, a th Herreras buclythe hot'd beed top ats roting Gad srisalas weakt he Prod le) stalters \n",
      "----\n",
      "20000 loss 51.7456364281\n",
      "----\n",
      " .\n",
      "Tk hist then seich tome full'' hat Snom llicplaing was the'd that done cos newurven,siacurnore the wat heong fehare. The bceannired,\" sarlinght better woondry mats. Heone cocmione, fenhe siall they  \n",
      "----\n",
      "21000 loss 52.9612652038\n",
      "----\n",
      " IMs(Qusim.\n",
      "The cowotes witheo thichany I seam?\"\n",
      "UnG Hbro lat buta lakekas it ofe somse a somasle wo ind thengoneve natthey rice jusirgers the growny heyeh gon?\" Hagridly yous lood fall th's grive oom. \n",
      "----\n",
      "22000 loss 52.5824560337\n",
      "----\n",
      " dfer nale; seopts mentth er th turable sonsim? dored cewellay bacref yinest, jewnole serreasin's qued, ave thatcoud?\"\n",
      "Yingmich in pormenve).\n",
      "\"Yicsert whas cough vand the way slabk oneh ow anitta theri \n",
      "----\n",
      "23000 loss 51.580783477\n",
      "----\n",
      "  a the\n",
      "\"Neary. Seyt doncoo Hegry, sirs, the wand ntwas ipist,\" Harry sapy becxorcant his of micsin Kasterper?\" sadne -- hasey a goigh to nutthe Do nit.\n",
      "\"S An'gadklumbow Re farn.\"\n",
      "\"Oh woand ourseed jow \n",
      "----\n",
      "24000 loss 51.6890206808\n",
      "----\n",
      " all beaple. Thor yugn saed the plot poy fasy feee wos tome was tare tit ouk.\n",
      "Oh\n",
      "\"Stape, aco \"Botthed staperst oat He youps thimarudthing, thead a wled a rerp--\". He firt,derearis thous Harjys operte a \n",
      "----\n",
      "25000 loss 51.8788345031\n",
      "----\n",
      " ouch He get? I ham maresthentery Hary potteiwhen shew of Hagry loom thy to the apmithe at becout akt ley muptery to hing potcebed nobled shici tood doof aylasles ape hiave aidrith wwer to to doud EI k \n",
      "----\n",
      "26000 loss 51.206965235\n",
      "----\n",
      " ght oheltsough ak yook but he --f Ron Fnes. do meet,. Irbass asf foreylu,\" hores asce, Fans fist bruiduth He rayon flott ord qyeaclit eats and in ceow, hright soujsaps exlatten Cramarl thescapind lay  \n",
      "----\n",
      "27000 loss 51.1760677053\n",
      "----\n",
      " 'r soy Band hearf.\"\n",
      "Her whaternd to mempecer.\n",
      "\"The whout'n'filss at.\n",
      "\"Midnghat' rotund tht Hardingain Homest ally (on wouduevinpat't a snatincafkenporS.\"\n",
      "You grim as--\"\n",
      "Proley.\n",
      "\n",
      "It he miste griesent,  \n",
      "----\n",
      "28000 loss 50.5887244649\n",
      "----\n",
      " .\"\n",
      "Thear fooks a k the fans one the --- walrt the that of lize ast Gryevederon get.\n",
      "\"Cout hom had tirshadi hemurnest hived tut Harry ta want onlapf,sy.\n",
      "Rang ouls --daite and evrotthontaik trach \"It Ro \n",
      "----\n",
      "29000 loss 50.430749291\n",
      "----\n",
      " t outt notherborteavist werry his fented ha iterair and's hat fall ithe a wins dinggeourwerringat russeas, all as'lway, got toke the ternot hed ofavis sird fice st he in. Higrid ham telld the atPcolks \n",
      "----\n",
      "30000 loss 50.1400182858\n",
      "----\n",
      " stiy pacilsfer -look had mired, wis itthas, getter thetered freeping.S\" foufthe was but mosnatted hat Sanithrioklyinge the he wels afwin Sfolle hip thinton'me leamenf ly,\" Harry nithe toor to Bempimid \n",
      "----\n",
      "31000 loss 50.0319469404\n",
      "----\n",
      "  daezed Hagrid,ornoth Boh)s ay aut Stewvin berams of -ing in exereg. Brim loys.\n",
      "Whas whutslts bidnit Durdesnaring, He slake turn!\" saidisto bothem ouding that'rid the in Lollom Hagridnce, deaged berus \n",
      "----\n",
      "32000 loss 50.0311882529\n",
      "----\n",
      " Bas werest they thoceling i do bost sis, uthous but, nastidaght oh wotre samoug Haddid. Nfmirkel Dumol ay he reariked at didly surilly sere there tay jbsombef shimalk bato the gris Harly bous an the w \n",
      "----\n",
      "33000 loss 49.4211307859\n",
      "----\n",
      " \"Donst We're anen and pook hwas inl bas to thespod to Rone tur to had Harrigoightingint, ano beoped thet -pEing on the gailn'llingt the 'nlize abutheyack it ound noud the sfa roming theand yoolhs. \"hi \n",
      "----\n",
      "34000 loss 49.4922379086\n",
      "----\n",
      " a.\n",
      "\"Yint.\"\n",
      "Harry walt if loorel erpered oploreg acrutey threagh youl home, hasing of beele. An stes.\". He thoudsethugSa goth mas like, bat petk the Hall gosely -he intarry was withing ano lithindor's  \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35000 loss 49.9602116771\n",
      "----\n",
      " is nemthas agf arry brot pooum er a carir, his the nach ser tur twning, I: Mp a sudgoor thrave -- he becket hae mille as couldapingh he the looked hed rage arry sthe nore storid slack yowar Harry was  \n",
      "----\n",
      "36000 loss 49.9534621879\n",
      "----\n",
      "  trifast he Ceisledn's satc--\"\n",
      "\"It withey. Pritser Prufferack het alys eget's rudn't rutpwas was Patr his Dimer up, iftiw not!\" sfort ?\"\n",
      "Harryqoorgedd''r Quedredild kentiould if Mha troud ot loukn are \n",
      "----\n",
      "37000 loss 49.7696268015\n",
      "----\n",
      " yold slobpteat agle mo Maver forre troustry.\n",
      "UPstranchanto \"Bunsto tho slag. Harry-f beettomes anchey Vrou thmoning Mo the weCn aivee any any fizars. Dudne do beyou thet the was, wepey!\"\n",
      "Harry loo was \n",
      "----\n",
      "38000 loss 50.4461217852\n",
      "----\n",
      " both hed tal at yruckle. \"Yaked. Dud Mod tien, ain. I knome enstaley, agoh, Hoor he Vor. EhG of ut fitthe than'l you watr yered' the for wot credm les, than'I the one be oke neard stheom ned goove tac \n",
      "----\n",
      "39000 loss 50.5215024785\n",
      "----\n",
      "  avingh the jumdleen Seapper chadetry was on you chouse anpmes sagned safcerary ied Dudlighing.\n",
      "Jury? And of thethore.\n",
      "Goncan'tlin Harry,  heThe did layas sidrion the soake th in beet it overingwhinge \n",
      "----\n",
      "40000 loss 50.055703201\n",
      "----\n",
      "  alver sext insizmed,.\"\n",
      "Hagrid nastoned.\"\n",
      "\"You hinet thiy he tar mayer they hone ithe undle hee was,\" Hatray andoway claid the a was Harryer Malf a ploith in? Migrand, one to wit fnet was the huche if \n",
      "----\n",
      "41000 loss 49.6691123004\n",
      "----\n",
      " y slowus moand atne Mrmig there.\n",
      "Aad, und, aldentand cefor.... Harry al nitilchling were ougre, Ide wayseh. Mucferando magaclliss, a thore to re~p shisver at bicke sow, an Hegbofe goat hrick anizarled \n",
      "----\n",
      "42000 loss 50.3290912682\n",
      "----\n",
      " r Sfle slooke ofe a\n",
      "\"Unst yins's, stercaws -ocer, Pxmimt.DAN\"ORBFEs!\" on virt of b!\"\n",
      "They and of up on pursen was filf at mull zormy mepat hickors!\" They stit in the wanling ttrow?\" Hagrill Prais wass \n",
      "----\n",
      "43000 loss 49.9133731394\n",
      "----\n",
      " e hand his alldien inding sailkneed shill shis fi'nted had hat and heringce oppory, digclacsnockVerniand sthe cawasr, hhe him! Arry gears toor arith oute'n Moflasdor liok thethe the he anst ass las as \n",
      "----\n",
      "44000 loss 49.4534824354\n",
      "----\n",
      " orpare ous fobasest, cear taf,yen'l to cronldele beind.\"\n",
      "Hlver tonou kin lying ht bleall shackid.\"\n",
      "\"Dong theg the cheoum tharsind nftyysed.\n",
      "\"MrRon, yous afkeddinge pom oo veat aust afly was. Pock Murr \n",
      "----\n",
      "45000 loss 49.3424416577\n",
      "----\n",
      " Cake ard a Snothed spid?\" Reace we,\" said and HemMard bromwas aitered everall Bnor me'n the euund and neseing then verry heind, theond a lidot!\" an rithe inbel off. \"Yout stell towin, diangeow. Mxenfi \n",
      "----\n",
      "46000 loss 49.5395143343\n",
      "----\n",
      " gand souk ton thes. AI mamepedbromwed Wiakn't sorrow.\n",
      "\"Geot' chatoutho monce was will.\n",
      "Ee stintwas that corof thintting couclin yous at waf on it beve wasteispiing a, anto ast? Ron as, moen pearfell b \n",
      "----\n",
      "47000 loss 48.763188543\n",
      "----\n",
      " thome forl otbeingor, Hor slicly he Flyts. Pmoffisiby.\n",
      "\"Pownes, whole. \"I a iff Filh?\n",
      "Andough ciaFtrall,.\"\n",
      "At on hiwneW?\" saieg in right whimled th corren watn'l hemured in tilly a coyters the int, mi \n",
      "----\n",
      "48000 loss 48.7792431189\n",
      "----\n",
      " resterped, there botto knowey les,ephis hitachaldon ppeyn'se at. thet sfelletariinste hat'rlobgaone wis unsting agaed he wowtine fe vered Mented Cand Ny; Prockeding hin would open thece. He cak fo qui \n",
      "----\n",
      "49000 loss 48.649141527\n",
      "----\n",
      " ngout ceppe. Nedlit'te cp otend?\" siind youd'le look o that'n listosteice. Foung -rais ent you do gane or him and eely i Gealk, -te fintoppery intaiof turow fehing at cat minderad you poomwing doo'mHi \n",
      "----\n",
      "50000 loss 48.2776818116\n",
      "----\n",
      " lostbe he hu so goomhen theted jumfiled sikee and this'ro fint loom of sugkting brait crellof fione clupget and t do bearco gricke alight with his not --mecnom the Loow.\n",
      "\"A bit'clage gointed out uleld \n",
      "----\n",
      "51000 loss 48.2623206087\n",
      "----\n",
      "  hish was in ham, and're, in tud lidrilthof my ume he guive the hisfsed. \"Oh otherref.... aser buck tcetaih I the o's sleoreh diwn alon's lusked one stiondertlaid Neh lettint. Ell in aT key.\n",
      "\"Ohat dad \n",
      "----\n",
      "52000 loss 48.8039345601\n",
      "----\n",
      " n thon stay wachasbe? The rard Sthe, anslay pum, in whoge, werven a Bougharinging how boitwerame on gep on) in hat athane trone sone aad, -- ceer oghog is the ble, Leep grinkll ur here thether. Neafe  \n",
      "----\n",
      "53000 loss 48.5940358494\n",
      "----\n",
      " erits walought and net ofolloice. Botoor doowding -- a hid a wiscould and it for. It that Harry an sack shatkey une coun mice. \"He fasts oneld. Nevalde the dediegce in as that tamfurm of alle. There i \n",
      "----\n",
      "54000 loss 48.7978795173\n",
      "----\n",
      "  ave furras as abbe. Chap to thedsiyeh up and.\n",
      "\"Yencos ther he stroundes hid lheper,nover, yfrupleing ill and ther that wouly his mur bealld lasem a be and to pamclot. Hardly cemsted tafirt.\n",
      "Hagrid lo \n",
      "----\n",
      "55000 loss 49.4626997927\n",
      "----\n",
      " othoron, war Hary. Ne?\" of ot aumn for -- hous heams coll,\"As --\"\n",
      "Harry. AnoI was intan'llenf.\" Quig't meord orene oover in cin's wand sedor thinngwin,\" he'roft, in to it Uncund ho gilly sing wall cra \n",
      "----\n",
      "56000 loss 49.9689786808\n",
      "----\n",
      " .\"\n",
      "I stinge, I've as ceavelus's you githing kse vie he fercly all same..\n",
      "\"The this mise! I'd parded troby ding,\" natters of the Lidderad she reagaatch at -- Mas call outtingly's had and aI to in --ins \n",
      "----\n",
      "57000 loss 49.439004354\n",
      "----\n",
      " 'gerhwagh he did Harry he was ankawn'tils. Snelot yepe b jumbouget hell'twed ard Voll. \"They toontone wald glofly a dop. Malf. I'm teU mach, saming itghisHe him of list,\n",
      "\"Ale catcbut yem, soun, not si \n",
      "----\n",
      "58000 loss 48.6219287043\n",
      "----\n",
      " ers, \"So kle o ferrige they in read's brenkit want farttere tand some Sto book hin. Haronp atteryed.\n",
      "That but beerbest tous till, a waldickleWa. \"Weted on trilput of nowine ur by boy stawish, Mr...\"\n",
      "\" \n",
      "----\n",
      "59000 loss 49.3424425914\n",
      "----\n",
      " redra your isap fin. Harry want fe ley hat to ere i bake yexppane.\n",
      "Peecoutthe pronens this han'd ee ploo copif twas the all, withafe watto grow hourle hod Hagregethated how.\n",
      "Havre iren africe perfs th \n",
      "----\n",
      "60000 loss 49.0678812173\n",
      "----\n",
      " s headrise becpcroor the like haf sars. I chly rofas wer? Anttous pqoicre ppose uvenforeing tor haind sainf winding bift'sortim the Gistermeshom the ofe ent rayfult alp whithanrouw dero mpin- huppishe \n",
      "----\n",
      "61000 loss 48.4709765744\n",
      "----\n",
      " ss cledoulle to cauld har the tingyeat tiRW Soverst, peloct rarch ow,\"Mith sered boolse there, hore pbowed for feare tobe to pnoneybut of a epat \"Quichey he roorhare iid thill a sund it to therrmisead \n",
      "----\n",
      "62000 loss 48.0218125094\n",
      "----\n",
      " d thead!\"\n",
      "Hafrysiras and tnoway ow thdould coldon the cherive we who Harry wounts out, ano tatchrien telf scerpome itting on a of and the divised the Ron siast hile seapling it tho gent rook thanse an \n",
      "----\n",
      "63000 loss 48.4977342757\n",
      "----\n",
      "  spiet p.\n",
      "Itry the then eld the snamt liatie's dewick Harry litht Harring Hagridanttreets all ch yous,- atteme wine an laspang talk. Howethor's hillets sill\"\n",
      "Horwin to su hamcheyed Hagryly Harry homet \n",
      "----\n",
      "64000 loss 47.718093733\n",
      "----\n",
      " kingavol DumTheling' marell oo suppe the sen waven tins.\n",
      "\"Yen.\n",
      "\"Thlikserer tobing ntaitsellingot the alf ie Srream,morew,\n",
      "TreptpeylaFme Malvwered ylughedy hing enking Frlithe'm Roniad a it abe as cong \n",
      "----\n",
      "65000 loss 47.8466847138\n",
      "----\n",
      " o Slakle,\" soid The knare bee to hed wa know in he delfor, a deared?\"\n",
      "\"We way. Noimed had the warkin hore waslose, dra's wise uple haver. \"Ron it -hey.\n",
      "\"Ch in thedriedo?\" seckert,in grecome clom mang  \n",
      "----\n",
      "66000 loss 47.9005976479\n",
      "----\n",
      " ns wend a and necally anit's wallook, amindonn oness was terpank hifleepidled acMusnowed anrrad. Harry any tut p waster. \"I Hagriditg\n",
      "Heb hiw sany ulins, dookbook one unidain ack --\" gacout by takised \n",
      "----\n",
      "67000 loss 47.5874036855\n",
      "----\n",
      "  to yee try sking a oud folvenore manount mourald -- ba theem.\n",
      "\"Nevoming gallher!\"\n",
      "Malffentan? Yous sthe fatt to meithaply, bot ago flead itainemed suireffize sulpers woted of the thetiled soarry stac \n",
      "----\n",
      "68000 loss 47.2489081096\n",
      "----\n",
      " ard o floms urey nound -- lascaped swell. yut baleepcourd whe will out inlwees, Mrsed and wits aliky Harry bnot waghtard naced, to cat wake...\"\n",
      "Hagridlee.\n",
      "Harry gwirn, thorkcon. I drat, Hiw elver himn \n",
      "----\n",
      "69000 loss 47.9056560962\n",
      "----\n",
      " terist the eald -f at thist book morts.\"\"Herriasen, fark we to Ron,\n",
      "Uncarid!\"\n",
      "\"Oh.\"\n",
      "Harry lispis sand beasen, to said hachested himas on, meadlischem.\n",
      "The to hish.\n",
      "\"Ghon, int: The hompelfor pom inting \n",
      "----\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70000 loss 47.9265752622\n",
      "----\n",
      " inged ceh it at himKeam walrey,.\" noachers ston.\n",
      "\"Wh'se laite pume oupttwastiled, inven biethey:\n",
      "IR Serly sore he oinhors leker asten treis had sabpight lewn, sooked?\"\n",
      "\"No bees urteakly lonke?\"\n",
      "\"The h \n",
      "----\n",
      "71000 loss 48.1626573477\n",
      "----\n",
      " Hething nitand. They'g, speoring the him seachast gown ver Ungstarter whang thionel haid!\"\n",
      "The fuat enthe ticknut whon'ts yaid Prone, hethach farded ick walbe in ittingiknhirs, McG-le hads at reagen t \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Real training\n",
    "\n",
    "n, position = 0,0\n",
    "\n",
    "# for Adaptive Gradient descent\n",
    "mWxh = np.zeros_like(Wxh);\n",
    "mWhh = np.zeros_like(Whh);\n",
    "mWhy = np.zeros_like(Why);\n",
    "mBxh = np.zeros_like(Bxh);\n",
    "mBhy = np.zeros_like(Bhy);\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length;\n",
    "\n",
    "epoch = 100*1000;\n",
    "sample_length = 200;\n",
    "\n",
    "while n<epoch:\n",
    "    \n",
    "    if(position+seq_length+1 >= len(data) or n == 0):\n",
    "        \n",
    "        hprev = np.zeros((hidden_size,1))\n",
    "        position = 0;\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "    #print(inputs)\n",
    "    targets = [char_to_ix[ch] for ch in data[position+2:position+seq_length+2]] \n",
    "    loss,dWxh,dWhh, dWhy, dBxh, dBhy, hprev = lossFunction(inputs,targets,hprev)  \n",
    "    smooth_loss = smooth_loss*0.999+loss*0.001\n",
    "    \n",
    "    if(n%1000 == 0):\n",
    "        print(n,\"loss\",smooth_loss)\n",
    "        sample(hprev,inputs[0],inputs[1],sample_length);\n",
    "        \n",
    "    # update\n",
    "    for param, dparam, mem in zip([Wxh,Whh,Why,Bxh,Bhy],\n",
    "                                  [dWxh,dWhh,dWhy,dBxh,dBhy],\n",
    "                                 [mWxh,mWhh,mWhy,mBxh,mBhy]):\n",
    "        \n",
    "        mem += dparam*dparam;\n",
    "        param += -learning_rate * dparam *  1 / np.sqrt(mem +1e-8) # Adagrad\n",
    "        \n",
    "    position += seq_length;\n",
    "    n += 1;\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
