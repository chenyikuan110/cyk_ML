{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "# Helper functions\n",
    "def softmax(array):\n",
    "    return np.exp(array)/ np.sum(np.exp(array)) # return an array\n",
    "\n",
    "def sigmoid(x):\n",
    "    return (1/(1+np.exp(-x)))\n",
    "\n",
    "def sigmoid_deriv(y):\n",
    "    return (y*(1-y))\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_deriv(y):\n",
    "    return 1 - pow(np.tanh(y),2)\n",
    "\n",
    "# RNN\n",
    "class basicRNN:\n",
    "    \n",
    "    def __init__ (self, lenIn, lenOut, lenRec, sizeHidden, inputs_encoded, targets, learningRate):\n",
    "        \n",
    "        # Hyper parameters\n",
    "        self.lenIn          = lenIn\n",
    "        self.lenOut         = lenOut\n",
    "        self.lenRec         = lenRec\n",
    "        self.sizeHidden     = sizeHidden\n",
    "        self.learningRate   = learningRate\n",
    "        \n",
    "        # input & expected output\n",
    "        self.inputs_encoded = inputs_encoded;\n",
    "        self.targets = targets;\n",
    "        \n",
    "        # parameters for inference\n",
    "        self.x  = np.zeros(lenIn)  \n",
    "        self.y  = np.zeros(lenOut)\n",
    "        self.h  = np.zeros(sizeHidden)\n",
    "        \n",
    "        self.W  = np.zeros((lenOut,sizeHidden)) # for the last fully connected layer        \n",
    "        self.b  = np.zeros(lenOut)\n",
    "        \n",
    "        \n",
    "        # for training phase \n",
    "        self.xs = np.zeros((lenRec,lenIn))\n",
    "        self.ys = np.zeros((lenRec,lenOut))\n",
    "        self.hs = np.zeros((lenRec,sizeHidden))\n",
    "        self.GW = np.zeros((lenOut,sizeHidden)) # Gradient, for W-update using RMSprop\n",
    "        self.Gb = np.zeros(lenOut)\n",
    "        \n",
    "        # CELL class\n",
    "        self.RNN_cell = RNN_cell(sizeHidden+lenIn,sizeHidden,lenRec,learningRate)\n",
    "        \n",
    "        ''' end of basicRNN.__init__ '''\n",
    "       \n",
    "    ''' This is used when mini-batch is used '''            \n",
    "    def update_inputs_targets(self, inputs_encoded, targets):\n",
    "        self.inputs_encoded  = inputs_encoded\n",
    "        self.targets         = targets\n",
    "    \n",
    "    def fwd_pass(self): \n",
    "        prev_h = np.zeros_like(self.hs[0])\n",
    "        for t in range(0,self.lenRec):\n",
    "            # update input\n",
    "            self.x    = self.inputs_encoded[t]\n",
    "            self.xs[t]= self.inputs_encoded[t]\n",
    "            \n",
    "            self.RNN_cell.hx = np.hstack((prev_h, self.x));\n",
    "           \n",
    "            h = self.RNN_cell.fwd_pass()\n",
    "            # bookkeeping\n",
    "            self.hs[t] = h\n",
    "            \n",
    "            # output layer - fully connected layer\n",
    "            self.ys[t] = np.dot(self.W,self.hs[t]) + self.b\n",
    "            prev_h = self.hs[t]\n",
    "            \n",
    "        return;              \n",
    "    \n",
    "    def bwd_pass(self):        \n",
    "\n",
    "        avg_loss = 0; # using cross entropy average\n",
    "        h2next_grad  = np.zeros(self.sizeHidden)\n",
    "        \n",
    "        # output bp\n",
    "        W_grad   = np.zeros((self.lenOut,self.sizeHidden))\n",
    "        b_grad  = np.zeros(self.lenOut)\n",
    "        hxW_grad  = np.zeros((self.sizeHidden,self.RNN_cell.lenIn));\n",
    "        hb_grad   = np.zeros((self.sizeHidden));\n",
    "                   \n",
    "        # propagates through time and layers\n",
    "\n",
    "        for t in reversed(range(0,self.lenRec)):\n",
    "            \n",
    "            prob = softmax(self.ys[t]) # prevent zero\n",
    "            prob_fix  = prob + 1e-9\n",
    "\n",
    "            # cross entropy\n",
    "            err       = np.log(prob_fix[self.targets[t]])\n",
    "            avg_loss += err\n",
    "     \n",
    "            dy = copy.deepcopy(prob)\n",
    "            dy[self.targets[t]] -= 1\n",
    "            \n",
    "            W_grad += np.dot((np.atleast_2d(dy)).T,np.atleast_2d(self.hs[t]))\n",
    "            b_grad += dy\n",
    "            \n",
    "            dh = np.dot(self.W.T,dy) + h2next_grad\n",
    "            \n",
    "            x_grad  = np.zeros(self.lenIn)\n",
    "            \n",
    "            if(t > 0):\n",
    "                prev_h = self.hs[t-1]\n",
    "            else:\n",
    "                prev_h = np.zeros_like(self.hs[0])\n",
    "                \n",
    "            self.RNN_cell.hx = np.hstack((prev_h,self.xs[t]))\n",
    "            self.RNN_cell.h  = self.hs[t]\n",
    "\n",
    "            dhxW, dhb, h2next_grad,x_grad = \\\n",
    "            self.RNN_cell.bwd_pass( dh );\n",
    "            \n",
    "            hxW_grad  +=  dhxW\n",
    "            hb_grad   +=  dhb\n",
    "            \n",
    "        self.RNN_cell.update(hxW_grad/self.lenRec, hb_grad/self.lenRec);\n",
    "        \n",
    "        self.update(W_grad/self.lenRec,b_grad/self.lenRec);\n",
    "        return avg_loss/self.lenRec;\n",
    "            \n",
    "          \n",
    "            \n",
    "    def update(self, W_grad, b_grad):\n",
    "        self.GW = self.GW + W_grad**2;\n",
    "        self.W -= self.learningRate/np.sqrt(self.GW + 1e-8) * W_grad;\n",
    "        self.Gb = self.Gb + b_grad**2;\n",
    "        self.b -= self.learningRate/np.sqrt(self.Gb + 1e-8) * b_grad;\n",
    "\n",
    "    def inference(self,x):\n",
    "        # update input\n",
    "        self.x = x\n",
    "        self.RNN_cell.hx = np.hstack((self.h,self.x))\n",
    "        self.h = self.RNN_cell.fwd_pass()\n",
    "\n",
    "        # output layer - may replace with softmax instead\n",
    "        self.y = np.dot(self.W,self.h) + self.b\n",
    "        p   = softmax(self.y)\n",
    "        \n",
    "        \n",
    "        return np.random.choice(range(self.lenIn), p=p.ravel())\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN_cell:\n",
    "    \n",
    "    def __init__ (self,lenIn,sizeHidden,lenRec,learningRate):\n",
    "        self.lenIn        = lenIn\n",
    "        self.sizeHidden   = sizeHidden\n",
    "        self.lenRec       = lenRec\n",
    "        self.learningRate = learningRate\n",
    "        \n",
    "        # hx == x is x and h horizontally stacked together\n",
    "        self.hx = np.zeros(lenIn)\n",
    "        self.h = np.zeros(sizeHidden)\n",
    "        \n",
    "        # Weight matrices\n",
    "        self.hxW = np.random.random((sizeHidden,lenIn));\n",
    "\n",
    "        # biases\n",
    "        self.hb = np.zeros(sizeHidden);\n",
    "              \n",
    "        # for RMSprop only\n",
    "        self.GhxW = np.random.random((sizeHidden,lenIn));\n",
    "        self.Ghb = np.zeros(sizeHidden);\n",
    "\n",
    "        \n",
    "        ''' end of RNN_cell.__init__ '''\n",
    "        \n",
    "    def fwd_pass(self):\n",
    "        self.h = tanh(np.dot(self.hxW, self.hx) + self.hb)       \n",
    "        return self.h;\n",
    "    \n",
    "    def bwd_pass(self, dh):\n",
    "        \n",
    "        #dh = np.clip(dh, -6, 6);       \n",
    "        # h = o*tanh(c)\n",
    "        dh  = tanh_deriv(self.h) * dh\n",
    "        dhb = dh\n",
    "        dhxW = np.dot((np.atleast_2d(dh)).T,np.atleast_2d(self.hx)) \n",
    "        \n",
    "        hx_grad = np.dot(self.hxW.T, dh)\n",
    "               \n",
    "        return dhxW, dhb, hx_grad[:self.sizeHidden],hx_grad[self.sizeHidden:];\n",
    "    \n",
    "    def update(self, hxW_grad, hb_grad):\n",
    "\n",
    "        self.GhxW = self.GhxW + hxW_grad**2\n",
    "        self.Ghb = self.Ghb + hb_grad**2\n",
    "        \n",
    "        self.hxW -= self.learningRate/np.sqrt(self.GhxW + 1e-8) * hxW_grad\n",
    "        self.hb -= self.learningRate/np.sqrt(self.Ghb + 1e-8) * hb_grad\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "431677 ,  79\n",
      "{'J': 0, ')': 1, 'I': 2, '-': 3, ' ': 4, 'w': 5, 'p': 6, '1': 7, '8': 8, ':': 9, '~': 10, 't': 11, '\\\\': 12, 'F': 13, '?': 14, 'Q': 15, '\\t': 16, 'O': 17, 'r': 18, 'H': 19, 'Z': 20, '!': 21, 'T': 22, 'q': 23, '6': 24, 'j': 25, 'L': 26, 'a': 27, 'y': 28, 'R': 29, 'W': 30, 'g': 31, 'G': 32, '4': 33, 'f': 34, '7': 35, '5': 36, 'P': 37, '\"': 38, 'e': 39, '2': 40, ';': 41, 'u': 42, 'K': 43, '.': 44, '*': 45, '(': 46, 'N': 47, '0': 48, 'z': 49, 'M': 50, '9': 51, 'X': 52, 'd': 53, 'o': 54, 'n': 55, 'h': 56, '\\n': 57, 'C': 58, 'A': 59, 'E': 60, 'V': 61, 'm': 62, 'i': 63, 'b': 64, 'v': 65, 'S': 66, 'x': 67, 'k': 68, 'B': 69, 'U': 70, '3': 71, ',': 72, 'c': 73, 's': 74, 'Y': 75, \"'\": 76, 'l': 77, 'D': 78}\n",
      "{0: 'J', 1: ')', 2: 'I', 3: '-', 4: ' ', 5: 'w', 6: 'p', 7: '1', 8: '8', 9: ':', 10: '~', 11: 't', 12: '\\\\', 13: 'F', 14: '?', 15: 'Q', 16: '\\t', 17: 'O', 18: 'r', 19: 'H', 20: 'Z', 21: '!', 22: 'T', 23: 'q', 24: '6', 25: 'j', 26: 'L', 27: 'a', 28: 'y', 29: 'R', 30: 'W', 31: 'g', 32: 'G', 33: '4', 34: 'f', 35: '7', 36: '5', 37: 'P', 38: '\"', 39: 'e', 40: '2', 41: ';', 42: 'u', 43: 'K', 44: '.', 45: '*', 46: '(', 47: 'N', 48: '0', 49: 'z', 50: 'M', 51: '9', 52: 'X', 53: 'd', 54: 'o', 55: 'n', 56: 'h', 57: '\\n', 58: 'C', 59: 'A', 60: 'E', 61: 'V', 62: 'm', 63: 'i', 64: 'b', 65: 'v', 66: 'S', 67: 'x', 68: 'k', 69: 'B', 70: 'U', 71: '3', 72: ',', 73: 'c', 74: 's', 75: 'Y', 76: \"'\", 77: 'l', 78: 'D'}\n",
      "Harry Potter and the Sorcerer's Stone\n",
      "CHAPTER ONE\n",
      "THE BOY WHO LIVED\n",
      "Mr. and\n",
      "inputs [19, 27, 18, 18, 28, 4, 37, 54, 11, 11, 39, 18, 4, 27, 55, 53, 4, 11, 56, 39, 4, 66, 54, 18, 73, 39, 18, 39, 18, 76, 74, 4, 66, 11, 54, 55, 39, 57, 58, 19, 59, 37, 22, 60, 29, 4, 17, 47, 60, 57, 22, 19, 60, 4, 69, 17, 75, 4, 30, 19, 17, 4, 26, 2, 61, 60, 78, 57, 50, 18, 44, 4, 27, 55, 53]\n",
      "arry Potter and the Sorcerer's Stone\n",
      "CHAPTER ONE\n",
      "THE BOY WHO LIVED\n",
      "Mr. and \n",
      "targets [27, 18, 18, 28, 4, 37, 54, 11, 11, 39, 18, 4, 27, 55, 53, 4, 11, 56, 39, 4, 66, 54, 18, 73, 39, 18, 39, 18, 76, 74, 4, 66, 11, 54, 55, 39, 57, 58, 19, 59, 37, 22, 60, 29, 4, 17, 47, 60, 57, 22, 19, 60, 4, 69, 17, 75, 4, 30, 19, 17, 4, 26, 2, 61, 60, 78, 57, 50, 18, 44, 4, 27, 55, 53, 4]\n",
      "!!!! 431677\n",
      "0 err: -4.36944777346702\n",
      "J\n",
      "BAdHL'TW.Sa\n",
      "IDN.\n",
      "s.WYSTMn.YCTInVtWINDeo \n",
      "1000 err: -3.01517784451819\n",
      "X\n",
      "i  a  eWel t  ?ess  noff.nn  e i seetro \n",
      "2000 err: -2.9726699254369953\n",
      "j\n",
      "dorb dorahhst,eonif r  i hohon '\n",
      "dk hh r\n",
      "3000 err: -2.961373430098563\n",
      "l\n",
      "u fsdd ss r rngd w e,hd Nuayheepsr dmoet\n",
      "4000 err: -3.274258379012928\n",
      "M\n",
      "\"e iiedsoe.cec liao ish  sectIdevia iHco\n",
      "5000 err: -3.2065098470002122\n",
      "q\n",
      "o\"i, ewysM.a.exoetwag wWo hs\n",
      "I\"onohdt\"\n",
      ":\n",
      "!!!! 431677\n",
      "6000 err: -3.0937104035625502\n",
      "Y\n",
      ".-\"esn.wwVdelahhm I o\"bD Mlsne laaa dP l\n",
      "7000 err: -4.165748032845989\n",
      "0\n",
      "aMyo  ia\n",
      "hYm RF,RteeS q EyAOdtlpscnznldO\n",
      "8000 err: -3.276356775183743\n",
      "!\n",
      "FabAeo isoWhsfP  \" cs\"lSgNpslhegssnH l t\n",
      "9000 err: -3.0454604537602084\n",
      "c\n",
      "lgyo hdufetuhr tdgpamnioiotalytuFe g gsf\n",
      "10000 err: -3.0780714275060896\n",
      "(\n",
      "io'le hm tlmia  lanoso woh .hunlaspl eyf\n",
      "11000 err: -3.1147281039257266\n",
      "H\n",
      "ctwfe\n",
      "y tiie o,ouwHHnwod tewu ee hsehd a\n",
      "!!!! 431677\n",
      "12000 err: -3.078130925580388\n",
      "3\n",
      "tn wit  hrwlc,en ,c.pathlrt Utn r atotg \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-7f89d2134a54>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     52\u001b[0m     \u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfwd_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 54\u001b[1;33m     \u001b[0merr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mR\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbwd_pass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     55\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-31-781b2334cc46>\u001b[0m in \u001b[0;36mbwd_pass\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRNN_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m             \u001b[0mdhxW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdhb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh2next_grad\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mx_grad\u001b[0m \u001b[1;33m=\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRNN_cell\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbwd_pass\u001b[0m\u001b[1;33m(\u001b[0m \u001b[0mdh\u001b[0m \u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m             \u001b[0mhxW_grad\u001b[0m  \u001b[1;33m+=\u001b[0m  \u001b[0mdhxW\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-32-9f715cca05d3>\u001b[0m in \u001b[0;36mbwd_pass\u001b[1;34m(self, dh)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0mdh\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtanh_deriv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[0mdhb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdh\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[0mdhxW\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0matleast_2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mhx_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhxW\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = open('HP1.txt','r', encoding=\"utf8\").read();\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(data_size,\", \",vocab_size)\n",
    "\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)\n",
    "\n",
    "def encode(idx,num_entry):\n",
    "    ret = np.zeros(num_entry)\n",
    "    ret[idx] = 1\n",
    "    return ret;\n",
    "\n",
    "def encode_array(array,num_entry):\n",
    "    xs = np.zeros((len(array),num_entry))\n",
    "    for i in range(len(array)):\n",
    "        xs[i][array[i]] = 1; \n",
    "    return xs;\n",
    "\n",
    "\n",
    "seq_length,position = 75,0\n",
    "inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "print(data[position:position+seq_length])\n",
    "print(\"inputs\",inputs)\n",
    "\n",
    "targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "print(data[position+1:position+seq_length+1])\n",
    "print(\"targets\",targets)\n",
    "\n",
    "n,position = 0,0;\n",
    "epoch = 20*1000;\n",
    "lenIn, lenOut, lenRec = vocab_size,vocab_size, seq_length\n",
    "sizeHidden, numHiddenLayer = 100,1;\n",
    "learningRate = 0.1;\n",
    "\n",
    "\n",
    "R = basicRNN(lenIn, lenOut, lenRec, sizeHidden, encode_array(inputs,vocab_size),targets, learningRate)\n",
    "\n",
    "# training\n",
    "while n<epoch:\n",
    "    \n",
    "    if(position+seq_length+1 >= len(data) or n == 0):\n",
    "        print(\"!!!!\",len(data))\n",
    "        position = 0;\n",
    "        \n",
    "    inputs  = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "\n",
    "    R.update_inputs_targets(encode_array(inputs,vocab_size),targets)\n",
    "    R.fwd_pass();\n",
    "    \n",
    "    err = R.bwd_pass();\n",
    "    \n",
    "    if(n%1000 == 0):\n",
    "        print(n,\"err:\",err)\n",
    "        seed = encode(n % vocab_size,vocab_size)\n",
    "        print(ix_to_char[n % vocab_size])\n",
    "        result = [];\n",
    "        R.h  = np.zeros(sizeHidden)\n",
    "        R.c  = np.zeros(sizeHidden)\n",
    "        for i in range(100):\n",
    "            ret = R.inference(seed)\n",
    "            #print(i,\":\",ret)\n",
    "            result.append(ret)\n",
    "            seed = encode(ret,vocab_size)\n",
    "        decode = ''.join([ix_to_char[ch] for ch in result] )\n",
    "        print(decode)\n",
    "\n",
    "    position += seq_length;\n",
    "    n += 1;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
