{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cudamat'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-5cc2b1e806f4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mcudamat\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'HP1.txt'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf8\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mchars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'cudamat'"
     ]
    }
   ],
   "source": [
    "import copy, numpy as np\n",
    "import cudamat as cm\n",
    "\n",
    "data = open('HP1.txt','r', encoding=\"utf8\").read();\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "print(data_size,\", \",vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'N': 0, 't': 1, 'O': 2, ';': 3, 'h': 4, '9': 5, 'X': 6, 'd': 7, 'T': 8, 'c': 9, 's': 10, '\\t': 11, '!': 12, 'E': 13, \"'\": 14, 'V': 15, 'J': 16, ':': 17, 'r': 18, 'j': 19, '~': 20, 'M': 21, '2': 22, 'P': 23, '\\n': 24, 'W': 25, 'z': 26, '\\\\': 27, '?': 28, 'Z': 29, '1': 30, 'a': 31, 'S': 32, 'L': 33, 'l': 34, 'b': 35, 'x': 36, 'U': 37, '7': 38, '*': 39, '5': 40, ',': 41, 'B': 42, 'v': 43, 'G': 44, 'e': 45, ' ': 46, 'u': 47, 'A': 48, 'i': 49, '4': 50, 'Q': 51, 'Y': 52, 'F': 53, 'g': 54, '.': 55, 'p': 56, 'w': 57, 'y': 58, ')': 59, '8': 60, '6': 61, 'f': 62, 'C': 63, 'R': 64, '(': 65, 'o': 66, 'D': 67, 'm': 68, '-': 69, 'H': 70, '\"': 71, 'q': 72, 'I': 73, '3': 74, 'n': 75, 'k': 76, '0': 77, 'K': 78}\n",
      "{0: 'N', 1: 't', 2: 'O', 3: ';', 4: 'h', 5: '9', 6: 'X', 7: 'd', 8: 'T', 9: 'c', 10: 's', 11: '\\t', 12: '!', 13: 'E', 14: \"'\", 15: 'V', 16: 'J', 17: ':', 18: 'r', 19: 'j', 20: '~', 21: 'M', 22: '2', 23: 'P', 24: '\\n', 25: 'W', 26: 'z', 27: '\\\\', 28: '?', 29: 'Z', 30: '1', 31: 'a', 32: 'S', 33: 'L', 34: 'l', 35: 'b', 36: 'x', 37: 'U', 38: '7', 39: '*', 40: '5', 41: ',', 42: 'B', 43: 'v', 44: 'G', 45: 'e', 46: ' ', 47: 'u', 48: 'A', 49: 'i', 50: '4', 51: 'Q', 52: 'Y', 53: 'F', 54: 'g', 55: '.', 56: 'p', 57: 'w', 58: 'y', 59: ')', 60: '8', 61: '6', 62: 'f', 63: 'C', 64: 'R', 65: '(', 66: 'o', 67: 'D', 68: 'm', 69: '-', 70: 'H', 71: '\"', 72: 'q', 73: 'I', 74: '3', 75: 'n', 76: 'k', 77: '0', 78: 'K'}\n"
     ]
    }
   ],
   "source": [
    "# Dictionary of input chars & indices\n",
    "char_to_ix = {ch:i for i, ch in enumerate(chars)}\n",
    "ix_to_char = {i:ch for i,ch in enumerate(chars)}\n",
    "print(char_to_ix)\n",
    "print(ix_to_char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "# demo of onehot encoding\n",
    "vector_for_char_a = np.zeros((vocab_size,1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1\n",
    "print(vector_for_char_a.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparams\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "# the lower the learning rate, the quicker the network abandons old belief for new input\n",
    "# e.g. train images on dogs, give a cat, low learning rate will consider cat is anormally rather than dog\n",
    "\n",
    "# Model params\n",
    "Wxh = np.random.randn(hidden_size,  vocab_size)* 0.01 # input to hidden (input is onehot encoded)\n",
    "Whh = np.random.randn(hidden_size, hidden_size)* 0.01 # recurrent hidden .\n",
    "Why = np.random.randn(vocab_size,  hidden_size)* 0.01 # hidden to output(decode the output)\n",
    "\n",
    "Bxh = np.zeros((hidden_size,1))\n",
    "Bhy = np.zeros((vocab_size,1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# softmax helper\n",
    "def softmax(seq):\n",
    "    return np.exp(seq)/ np.sum(np.exp(seq))\n",
    "\n",
    "def softmax_array(two_D_seq,t):\n",
    "    return np.exp(two_D_seq[t])/ np.sum(np.exp(two_D_seq[t]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Loss function - training\n",
    "def lossFunction(inputs, targets, prev_hidden):\n",
    "    # p is softmax probability\n",
    "    xs, hs, ys, ps = {},{},{},{};\n",
    "    \n",
    "    hs[-1] = copy.deepcopy(prev_hidden)\n",
    "    loss = 0;\n",
    "    \n",
    "    # Fwd pass    \n",
    "    for t in range(len(inputs)):\n",
    "        # One hot encoding for the input char using our dictionary\n",
    "        xs[t] = np.zeros((vocab_size,1));\n",
    "        xs[t][inputs[t]] = 1; \n",
    "        \n",
    "        hs[t] = np.tanh(np.dot(Wxh,xs[t]) + np.dot(Whh,hs[t-1]) + Bxh);\n",
    "        ys[t] = np.dot(Why,hs[t]) + Bhy;\n",
    "        ps[t] = softmax_array(ys,t);\n",
    "        char_idx = targets[t]\n",
    "        loss += -np.log(ps[t][char_idx,0]) \n",
    "        # ps[t][targets[t]] is the prob. node corrs. to. t_th char in the label array\n",
    "\n",
    "        \n",
    "    # Gradient value holders\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh),np.zeros_like(Whh),np.zeros_like(Why)\n",
    "    dBxh, dBhy = np.zeros_like(Bxh),np.zeros_like(Bhy)\n",
    "    dhnext = np.zeros_like(hs[0])\n",
    "    \n",
    "    # Bwd pass\n",
    "    for t in reversed(range(len(inputs))):\n",
    "        dy = copy.deepcopy(ps[t])\n",
    "        dy[targets[t]] -= 1 # this is how we calculate loss using onehot encoding\n",
    "         \n",
    "        dWhy += np.dot(dy, hs[t].T);\n",
    "        \n",
    "        dBhy += dy # derivative w.r.t bias is 1\n",
    "        \n",
    "        dh = np.dot(Why.T,dy) + dhnext # back prop the error from y into h\n",
    "        dhraw = (1-hs[t]*hs[t])*dh # back prop thru tanh\n",
    "        \n",
    "        dBxh += dhraw  # derivative of Wx+b w.r.t b is 1; d_loss/d_b = d_loss/d_H * d_H/d_b\n",
    "        dWxh += np.dot(dhraw, xs[t].T) # derivative of Wx+b w.r.t W is x\n",
    "        dWhh += np.dot(dhraw,hs[t-1].T)\n",
    "        dhnext = np.dot(Whh.T, dhraw)\n",
    "\n",
    "    # Can be replaced using LSTM structure\n",
    "    for dparam in [dWxh, dWhh, dWhy, dBxh, dBhy]:\n",
    "        np.clip(dparam,-5,5,out=dparam) # mitigate gradient vanish\n",
    "        \n",
    "        \n",
    "    return loss,dWxh,dWhh, dWhy, dBxh, dBhy, hs[len(inputs)-1];\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prediction\n",
    "def sample(h,seed_ix,n):\n",
    "    x = np.zeros((vocab_size,1))\n",
    "    # one hot encode\n",
    "    x[seed_ix] = 1;\n",
    "    ixes = [] # empty sentence\n",
    "\n",
    "    for t in range(n):\n",
    "\n",
    "        h = np.tanh(np.dot(Wxh,x) + np.dot(Whh,h) + Bxh);\n",
    "        y = np.dot(Why,h) + Bhy;\n",
    "        p = softmax(y);\n",
    "\n",
    "        # sample the output\n",
    "        ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "        \n",
    "        # encode this output\n",
    "        x = np.zeros((vocab_size,1))\n",
    "        x[ix] = 1;\n",
    "\n",
    "        ixes.append(ix)\n",
    "        # if n > 1, it will predict more than 1 subsequent chars\n",
    "        \n",
    "    txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "    print (\"----\\n %s \\n----\" % (txt,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Harry Potter and the Sorc\n",
      "inputs [70, 31, 18, 18, 58, 46, 23, 66, 1, 1, 45, 18, 46, 31, 75, 7, 46, 1, 4, 45, 46, 32, 66, 18, 9]\n",
      "arry Potter and the Sorce\n",
      "targets [31, 18, 18, 58, 46, 23, 66, 1, 1, 45, 18, 46, 31, 75, 7, 46, 1, 4, 45, 46, 32, 66, 18, 9, 45]\n"
     ]
    }
   ],
   "source": [
    "# Training using Adagrad (decreasing learning rate)\n",
    "position = 0\n",
    "inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "print(data[position:position+seq_length])\n",
    "print(\"inputs\",inputs)\n",
    "targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "print(data[position+1:position+seq_length+1])\n",
    "print(\"targets\",targets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 109.236196764\n",
      "----\n",
      " 71mRrsDnVtcFsz-eI,-IR\n",
      "\t24Lb8iYbsvB\"OrG7Os95QfI6H'asOjiaYCcWNxNjPjA1tvn0-(Yap2YL,YXj;t?MsbQ\\0Idd)comEhTc\n",
      "vOF4G:XzggUY-a(?JYz8rD6Q?H\tLW7\n",
      "1QT,7 BxwBzuaz7KiyaDrSl9Vq8KWEp7'U)I6wj)yq \n",
      "mbYnphRcn n9'~X\"~4EA1 \n",
      "----\n",
      "1000 loss 85.6394419564\n",
      "----\n",
      "  taove thstdatsotaecat tonttsimbeld\n",
      "e  anild Dnt nile thovenl wisse soy wpalg hind aref rby thacsrintnasdey plf sicG, rir neyereopeca-e yirilt onessocafe held sisektocivevett yes cinGonhorelyyer yreor \n",
      "----\n",
      "2000 loss 70.7535694981\n",
      "----\n",
      " he w. the biss\n",
      " har one word ppoa cowthame hemelt boolacphethethe dloed yorimls hethe pDud,y bipper lopry he Voale carthis, angwathim-ann. hher an wad he wha he ms wo the  his wasrey vaspewwivit hnon  \n",
      "----\n",
      "3000 loss 63.6919067969\n",
      "----\n",
      " d uthict thy as ths outcin ass shis how home onstatuntto ffry con, sut ce Vot sasped bthino dop cu lo his ta interes red- tt bre wrof tinger'n athowstam tor thiswhe vifh qfo boas stat aanthanca tampin \n",
      "----\n",
      "4000 loss 60.8374273035\n",
      "----\n",
      " ay oy mis t of un,.wDnam Igs humy.\" Ta cay at er caevere t lous en hs o cayed on l, ly yakr. nerblay opevey as stof ta pall, s, mor, Afid and, ker than themaisep\" yom \"Y'be ono prawneey. The kizS Lock \n",
      "----\n",
      "5000 loss 57.9920822096\n",
      "----\n",
      " o the the leetinthrok in Un und grer, kem hucg't, yoor thiston thl the mang fcRUnt, cher the he puikes qulen\"\"The dit in bon sough, I lits,ion mle.\n",
      "Thank, knid, on anes thes nas thom inmern the bil li \n",
      "----\n",
      "6000 loss 56.1186561448\n",
      "----\n",
      " hes asplot?\" said.ses dey,\"\n",
      "\"Sois sis sale. Shes aldand wcoust, Gr-be aoG bowh? bre louzest amigd \"Fefwe thevestapbead youp ary yaf thaseelid' farly thatsed. A--, Dut bawfodeme Girnte hee thas tilpuld \n",
      "----\n",
      "7000 loss 55.7403236453\n",
      "----\n",
      "  Nese ofrellen toid Hiseay,finky I gove hy yead hishelloges fEtut sa'veadu goc ouprlyther'avery wOrcoor oubamery openor fall wat ceroud toy siin of heming ror, and, suulhedilL wandoaclorfers meto warr \n",
      "----\n",
      "8000 loss 55.0680270962\n",
      "----\n",
      "  onss torke. Auscestsbange whouplll, Ind agouee cactid Mxkpuin Qulk. Megingleoukfe's toll and tor tuscagllling inettsancossed ROf so and thail Rus a phane logst Gevit weting trlitslyongagefass hQueter \n",
      "----\n",
      "9000 loss 53.8001078353\n",
      "----\n",
      " re! Fusw, they farcasthe thare\"\n",
      "Hhey Flo kly chenmpead,scop, hiin, stoad scarle sied roare? He so'thist ele.\n",
      "Ne coundyle thayer the meas Morits aby dored ack, Harrbonounts wanded the shack heddrow ber \n",
      "----\n",
      "10000 loss 52.504030764\n",
      "----\n",
      " \n",
      "\"\n",
      "\"You and loom the hede't was freanidemminged slitwing,\" he ssee omary.\n",
      "\"Vesitcert't reat midf Mf bad.\"\n",
      "\"Chat fouth nead athindut Hagly.\n",
      "It nof to had me,manded Mf got the dave sainitreat. Che they. \n",
      "----\n",
      "11000 loss 52.6599466331\n",
      "----\n",
      " . \"Co pear from be ofrly withdild ated sfoyefo io rithessos I thry, lame didt heod utems grop erearind yee. \"Werk thetring tire arry, as ur sive thatrrough toollronksthe Sulry, inA\"\n",
      "\"Intalf Messaok. N \n",
      "----\n",
      "12000 loss 51.5435238574\n",
      "----\n",
      " istalle intharere hvin I snatingruded. \"Chot tonyly. \"Will dirhtomam the rorn as Un gemin't erething CAs.\"\n",
      "Rung hilked and you diss to exiced, Opave staided ren,\" saiclazire Durthey a hirriald Finould \n",
      "----\n",
      "13000 loss 51.6054337152\n",
      "----\n",
      " ing ware gighid drling wen's out. \"Bowing a Chat,\" taid- alr, Ie as at staid. Whougd ar? \"Daty?s a- on mmon \"I'l mannth ter a theaine suck a mut Pintt'rn Quasitt steatardor and coog't den'ssande tuge, \n",
      "----\n",
      "14000 loss 50.9848012389\n",
      "----\n",
      " g.they damoru'ting?\"\n",
      "\"Yot eeore thes ufoulds the got oon's ta swes fope theind th'me didfedsaddboppitt be wout, fredlean I this ut solned he fean seir anad, but huike the doina don teace. Tintteareit  \n",
      "----\n",
      "15000 loss 50.7018784262\n",
      "----\n",
      " 't hidninwed toricesly was y-- Dudde's he ulpieted tant dumbum. Os in exed distonares wack fan auved theme't shoor, Herry tu did Potcometo didPort. \"Ssweak.\n",
      "Sever -- Hagridtsere,\n",
      "\"Po aunded.\"\n",
      "se thad  \n",
      "----\n",
      "16000 loss 49.7412079063\n",
      "----\n",
      " ve?\"\n",
      "\"When por then,,\" Nay wabcer unch; on GUsth rof --\" hildinghar to hut bett'r wnaele flackboy you you trred, to thad puginking at tomablowry arlyed lo werb deas: now look.\n",
      "\"You urd rum in quistall \n",
      "----\n",
      "17000 loss 50.0786220525\n",
      "----\n",
      " dluch Harryto hatroy hastreesled Harry fere He, up en the way at he gofed!\"\n",
      "\"Thith as un. yot lustremore whard... \"Hat't Hat was shan he whive nife sile alrGl he'mbuobagale.\n",
      "Thethando awly boouth Her  \n",
      "----\n",
      "18000 loss 53.6959650721\n",
      "----\n",
      " r..\n",
      "\"Dulider'g't --le sing dound thonadlyas.\"\n",
      "\"sed't pee's as wive stumf mimsso fors.rF pouy? Piensing pult sudly,-y ask't hinkals -- dhige be,\" sarkley't'ge any He ce Vmot nouglly nor'rthad anded.\"\n",
      "G \n",
      "----\n",
      "19000 loss 53.6460746484\n",
      "----\n",
      " is bok.... Unatt matlyen codecadedstad tarr righie.\n",
      "Arry to ting --e up had ake he dot fim. Ancrerss cheving his.\n",
      "Her sas, the spe't he at a and wisnce scou drembean a sing!\" a tare a rarry?\"Ytoulerou \n",
      "----\n",
      "20000 loss 52.8512797711\n",
      "----\n",
      " ng.\n",
      "I jantwrough maked juat it geen caud.HThat hatr\" jepurth oo ser, was stupt oKno fock hes. \"Fas bealle seipe. He ble codryon, heme,\"\n",
      "\"We and but smound -- be wistchorsed on Has a sonl!\"\n",
      "Thay, inats \n",
      "----\n",
      "21000 loss 54.0197574431\n",
      "----\n",
      " s y ife stast dant, Wift Grent dorn gand tars rak)ilt wasting eI: Hers pas wat thereeh aOncraMkizelg cot belboevo Theybert (On Pabl fhite simars grantt anarant Layters:\n",
      "THArliale: I drwad S wane lott  \n",
      "----\n",
      "22000 loss 53.2424811151\n",
      "----\n",
      " hens Unchumghil to feel hiller.\"\n",
      "\"I siinry the ere sco,, jousn' theousl Mritoreardor cad, mmofinthocluirs oncher theind yot.\n",
      "\"Thew hadring plink seed the in, cand on sruich,\"\n",
      "\"Mn was had. \"en rearle a \n",
      "----\n",
      "23000 loss 52.1231691285\n",
      "----\n",
      " o cald that Cain tewaring we Reh toradorn'o hay on buset, anu'memice his of a bettinbingmarry fot nat Duvel What but ofat paats, hio to arin Med, but to be samp, one wheve,\" heanvernconefalattinkle\n",
      "\"s \n",
      "----\n",
      "24000 loss 52.1390104654\n",
      "----\n",
      " is sasge0 os Grint ill ffut, we rave fheyy hoicheyserit. Houk eferted.\n",
      "\"Woy hichas fire.\n",
      "Harrhdsert sor...\"Plact if a be, Rth.\n",
      "\"That a bears. \"orcole. she?\n",
      "Ho hirs. He wisew of thet yof ow. stiff newa \n",
      "----\n",
      "25000 loss 52.3145902408\n",
      "----\n",
      " en, er of wim caudhed cors, ind chadion. T thending formidsledssed and sundhive hepper. The fit fon the worraksoom. Ar dim, Harry plam Houle ofn't leon phatilllomilage, wifeared of a Wome nover wanded \n",
      "----\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-8d85e21ce8fd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchar_to_ix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[0mtargets\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mchar_to_ix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mch\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mposition\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m     \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWxh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdWhh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdWhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdBxh\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdBhy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlossFunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtargets\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhprev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m     \u001b[0msmooth_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msmooth_loss\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.999\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-7-4cc56aae5e51>\u001b[0m in \u001b[0;36mlossFunction\u001b[1;34m(inputs, targets, prev_hidden)\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtanh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWxh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhh\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBxh\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mys\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mWhy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mBhy\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mps\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msoftmax_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mys\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m;\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Real training\n",
    "\n",
    "n, position = 0,0\n",
    "\n",
    "# for Adaptive Gradient descent\n",
    "mWxh = np.zeros_like(Wxh);\n",
    "mWhh = np.zeros_like(Whh);\n",
    "mWhy = np.zeros_like(Why);\n",
    "mBxh = np.zeros_like(Bxh);\n",
    "mBhy = np.zeros_like(Bhy);\n",
    "\n",
    "smooth_loss = -np.log(1.0/vocab_size)*seq_length;\n",
    "\n",
    "epoch = 100*1000;\n",
    "sample_length = 200;\n",
    "\n",
    "while n<epoch:\n",
    "    \n",
    "    if(position+seq_length+1 >= len(data) or n == 0):\n",
    "        \n",
    "        hprev = np.zeros((hidden_size,1))\n",
    "        position = 0;\n",
    "        \n",
    "    inputs = [char_to_ix[ch] for ch in data[position:position+seq_length]]\n",
    "    targets = [char_to_ix[ch] for ch in data[position+1:position+seq_length+1]] \n",
    "    loss,dWxh,dWhh, dWhy, dBxh, dBhy, hprev = lossFunction(inputs,targets,hprev)  \n",
    "    smooth_loss = smooth_loss*0.999+loss*0.001\n",
    "    \n",
    "    if(n%1000 == 0):\n",
    "        print(n,\"loss\",smooth_loss)\n",
    "        sample(hprev,inputs[0],sample_length);\n",
    "        \n",
    "    # update\n",
    "    for param, dparam, mem in zip([Wxh,Whh,Why,Bxh,Bhy],\n",
    "                                  [dWxh,dWhh,dWhy,dBxh,dBhy],\n",
    "                                 [mWxh,mWhh,mWhy,mBxh,mBhy]):\n",
    "        \n",
    "        mem += dparam*dparam;\n",
    "        param += -learning_rate * dparam *  1 / np.sqrt(mem +1e-8) # Adagrad\n",
    "        \n",
    "    position += seq_length;\n",
    "    n += 1;\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
